{
    "docs": [
        {
            "location": "/", 
            "text": "UncertainData.jl\n\n\n\n\nMotivation\n\n\nUncertainData.jl was born to systematically deal with uncertain data, and to sample uncertain dataset more rigorously. It makes workflows involving uncertain data significantly easier.\n\n\n\n\nProbabilistic representation of uncertain observations\n\n\nWay too often in data analysis the uncertainties in observational data are ignored or not dealt with in a systematic manner. The core concept of the package is that uncertain data should live in the probability domain, not as single value representations of the data (e.g. the mean).\n\n\n\n\nUncertain values and datasets of uncertain values\n\n\nIn this package, data values are stored as probability distributions. Individual uncertain observations may be collected in \nUncertainDatasets\n, which can be sampled according to user-provided sampling constraints.\n\n\nLikewise, indices (e.g. time, depth or any other index) of observations are also represented as probability distributions. Indices may also be sampled using constraints, for example enforcing strictly increasing values.\n\n\n\n\nBasic workflow\n\n\n\n\nCreating uncertain values\n\n\nUncertain values are created by using the \nUncertainValue\n constructor.\n\n\nThere are currently three ways to construct uncertain values:\n\n\n\n\nEstimating the distribution of your data using kernel density estimation.\n\n\nFitting a distribution to empirical data (if you know roughly what type  of distribution is appropriate).\n\n\nSpecifying a probability distribution with known parameters (if you want  to represent data found in the literature, for example normally distributed  values with some standard deviation).\n\n\n\n\n\n\nResampling\n\n\nUncertain values may be resampled using the \nresample(uv::UncertainValue)\n function, which has methods for all the different types of uncertain values.", 
            "title": "Home"
        }, 
        {
            "location": "/#uncertaindatajl", 
            "text": "", 
            "title": "UncertainData.jl"
        }, 
        {
            "location": "/#motivation", 
            "text": "UncertainData.jl was born to systematically deal with uncertain data, and to sample uncertain dataset more rigorously. It makes workflows involving uncertain data significantly easier.", 
            "title": "Motivation"
        }, 
        {
            "location": "/#probabilistic_representation_of_uncertain_observations", 
            "text": "Way too often in data analysis the uncertainties in observational data are ignored or not dealt with in a systematic manner. The core concept of the package is that uncertain data should live in the probability domain, not as single value representations of the data (e.g. the mean).", 
            "title": "Probabilistic representation of uncertain observations"
        }, 
        {
            "location": "/#uncertain_values_and_datasets_of_uncertain_values", 
            "text": "In this package, data values are stored as probability distributions. Individual uncertain observations may be collected in  UncertainDatasets , which can be sampled according to user-provided sampling constraints.  Likewise, indices (e.g. time, depth or any other index) of observations are also represented as probability distributions. Indices may also be sampled using constraints, for example enforcing strictly increasing values.", 
            "title": "Uncertain values and datasets of uncertain values"
        }, 
        {
            "location": "/#basic_workflow", 
            "text": "", 
            "title": "Basic workflow"
        }, 
        {
            "location": "/#creating_uncertain_values", 
            "text": "Uncertain values are created by using the  UncertainValue  constructor.  There are currently three ways to construct uncertain values:   Estimating the distribution of your data using kernel density estimation.  Fitting a distribution to empirical data (if you know roughly what type  of distribution is appropriate).  Specifying a probability distribution with known parameters (if you want  to represent data found in the literature, for example normally distributed  values with some standard deviation).", 
            "title": "Creating uncertain values"
        }, 
        {
            "location": "/#resampling", 
            "text": "Uncertain values may be resampled using the  resample(uv::UncertainValue)  function, which has methods for all the different types of uncertain values.", 
            "title": "Resampling"
        }, 
        {
            "location": "/uncertainvalues_overview/", 
            "text": "Uncertain values may be constructed in three different ways, depending on what information you have available. You may represent an uncertain value by\n\n\n\n\ntheoretical distributions with known parameters\n\n\ntheoretical distributions with parameters fitted to empirical data\n\n\nkernel density estimates to empirical data\n\n\n\n\n\n\nExamples\n\n\nIf the data doesn't follow an obvious theoretical distribution, the recommended course of action is to represent the uncertain value with a kernel density estimate of the distribution.\n\n\n\n\n\n\nImplicit KDE estimate\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nusing\n \nDistributions\n,\n \nUncertainData\n,\n \nKernelDensity\n\n\n\n# Generate some random data from a normal distribution, so that we get a\n\n\n# histogram resembling a normal distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nNormal\n(),\n \n1000\n)\n\n\n\n# Uncertain value represented by a kernel density estimate (it is inferred\n\n\n# that KDE is wanted when no distribution is provided to the constructor).\n\n\nuv\n \n=\n \nUncertainValue\n(\nsome_sample\n)\n\n\n\n\n\n\n\n\nExplicit KDE estimate\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some random data from a normal distribution, so that we get a\n\n\n# histogram resembling a normal distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nNormal\n(),\n \n1000\n)\n\n\n\n\n\n# Specify that we want a kernel density estimate representation\n\n\nuv\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\nIf your data has a histogram closely resembling some theoretical distribution, the uncertain value may be represented by fitting such a distribution to the data.\n\n\n\n\n\n\nExample 1: fitting a normal distribution\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some random data from a normal distribution, so that we get a\n\n\n# histogram resembling a normal distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nNormal\n(),\n \n1000\n)\n\n\n\n# Uncertain value represented by a theoretical normal distribution with\n\n\n# parameters fitted to the data.\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\nExample 2: fitting a gamma distribution\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate some random data from a gamma distribution, so that we get a\n\n\n# histogram resembling a gamma distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nGamma\n(),\n \n1000\n)\n\n\n\n# Uncertain value represented by a theoretical gamma distribution with\n\n\n# parameters fitted to the data.\n\n\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\nIt is common when working with uncertain data found in the scientific literature that data value are stated to follow a distribution with given parameters. For example, a data value may be given as normal distribution with a given mean \n\u03bc = 2.2\n and standard deviation \n\u03c3 = 0.3\n.\n\n\n\n\n\n\nExample 1: theoretical normal distribution\n\n\n1\n2\n3\n# Uncertain value represented by a theoretical normal distribution with\n\n\n# known parameters \u03bc = 2.2 and \u03c3 = 0.3\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n2.2\n,\n \n0.3\n)\n\n\n\n\n\n\n\n\nExample 2: theoretical gamma distribution\n\n\n1\n2\n3\n# Uncertain value represented by a theoretical gamma distribution with\n\n\n# known parameters \u03b1 = 2.1 and \u03b8 = 3.1\n\n\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n2.1\n,\n \n3.1\n)\n\n\n\n\n\n\n\n\nExample 3: theoretical binomial distribution\n\n\n1\n2\n3\n# Uncertain value represented by a theoretical binomial distribution with\n\n\n# known parameters p = 32 and p = 0.13\n\n\nuv\n \n=\n \nUncertainValue\n(\nBinomial\n,\n \n32\n,\n \n0.13\n)", 
            "title": "Quickstart"
        }, 
        {
            "location": "/uncertainvalues_overview/#examples", 
            "text": "If the data doesn't follow an obvious theoretical distribution, the recommended course of action is to represent the uncertain value with a kernel density estimate of the distribution.    Implicit KDE estimate  1\n2\n3\n4\n5\n6\n7\n8\n9 using   Distributions ,   UncertainData ,   KernelDensity  # Generate some random data from a normal distribution, so that we get a  # histogram resembling a normal distribution.  some_sample   =   rand ( Normal (),   1000 )  # Uncertain value represented by a kernel density estimate (it is inferred  # that KDE is wanted when no distribution is provided to the constructor).  uv   =   UncertainValue ( some_sample )     Explicit KDE estimate   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 using   Distributions ,   UncertainData  # Generate some random data from a normal distribution, so that we get a  # histogram resembling a normal distribution.  some_sample   =   rand ( Normal (),   1000 )  # Specify that we want a kernel density estimate representation  uv   =   UncertainValue ( UnivariateKDE ,   some_sample )     If your data has a histogram closely resembling some theoretical distribution, the uncertain value may be represented by fitting such a distribution to the data.    Example 1: fitting a normal distribution  1\n2\n3\n4\n5\n6\n7\n8\n9 using   Distributions ,   UncertainData  # Generate some random data from a normal distribution, so that we get a  # histogram resembling a normal distribution.  some_sample   =   rand ( Normal (),   1000 )  # Uncertain value represented by a theoretical normal distribution with  # parameters fitted to the data.  uv   =   UncertainValue ( Normal ,   some_sample )     Example 2: fitting a gamma distribution  1\n2\n3\n4\n5\n6\n7\n8\n9 using   Distributions ,   UncertainData  # Generate some random data from a gamma distribution, so that we get a  # histogram resembling a gamma distribution.  some_sample   =   rand ( Gamma (),   1000 )  # Uncertain value represented by a theoretical gamma distribution with  # parameters fitted to the data.  uv   =   UncertainValue ( Gamma ,   some_sample )     It is common when working with uncertain data found in the scientific literature that data value are stated to follow a distribution with given parameters. For example, a data value may be given as normal distribution with a given mean  \u03bc = 2.2  and standard deviation  \u03c3 = 0.3 .    Example 1: theoretical normal distribution  1\n2\n3 # Uncertain value represented by a theoretical normal distribution with  # known parameters \u03bc = 2.2 and \u03c3 = 0.3  uv   =   UncertainValue ( Normal ,   2.2 ,   0.3 )     Example 2: theoretical gamma distribution  1\n2\n3 # Uncertain value represented by a theoretical gamma distribution with  # known parameters \u03b1 = 2.1 and \u03b8 = 3.1  uv   =   UncertainValue ( Gamma ,   2.1 ,   3.1 )     Example 3: theoretical binomial distribution  1\n2\n3 # Uncertain value represented by a theoretical binomial distribution with  # known parameters p = 32 and p = 0.13  uv   =   UncertainValue ( Binomial ,   32 ,   0.13 )", 
            "title": "Examples"
        }, 
        {
            "location": "/uncertainvalues_kde/", 
            "text": "When your data have an empirical distribution that doesn't follow any obvious theoretical distribution, the data may be represented by a kernel density estimate.\n\n\n\n\nExamples\n\n\n\n\n\n\nImplicit KDE constructor\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nNormal\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Use the implicit KDE constructor to create the uncertain value\n\n\nuv\n \n=\n \nUncertainValue\n(\nv\n::\nVector\n)\n\n\n\n\n\n\n\n\nExplicit KDE constructor\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nusing\n \nDistributions\n,\n \nUncertainData\n,\n \nKernelDensity\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nNormal\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Use the explicit KDE constructor to create the uncertain value.\n\n\n# This constructor follows the same convention as when fitting distributions\n\n\n# to empirical data, so this is the recommended way to construct KDE estimates.\n\n\nuv\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nv\n::\nVector\n)\n\n\n\n\n\n\n\n\nChanging the kernel\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nusing\n \nDistributions\n,\n \nUncertainData\n,\n \nKernelDensity\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nNormal\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Use the explicit KDE constructor to create the uncertain value, specifying\n\n\n# that we want to use normal distributions as the kernel. The kernel can be\n\n\n# any valid kernel from Distributions.jl, and the default is to use normal\n\n\n# distributions.\n\n\nuv\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nv\n::\nVector\n;\n \nkernel\n \n=\n \nNormal\n)\n\n\n\n\n\n\n\n\nAdjusting number of points\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nusing\n \nDistributions\n,\n \nUncertainData\n,\n \nKernelDensity\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nNormal\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Use the explicit KDE constructor to create the uncertain value, specifying\n\n\n# the number of points we want to use for the kernel density estimate. Fast\n\n\n# Fourier transforms are used behind the scenes, so the number of points\n\n\n# should be a power of 2 (the default is 2048 points).\n\n\nuv\n \n=\n \nUncertainValue\n(\nUnivariateKDE\n,\n \nv\n::\nVector\n;\n \nnpoints\n \n=\n \n1024\n)\n\n\n\n\n\n\n\n\n\n\nExtended example\n\n\nLet's create a bimodal distribution, then sample 10000 values from it.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nusing\n \nDistributions\n\n\n\nn1\n \n=\n \nNormal\n(\n-\n3.0\n,\n \n1.2\n)\n\n\nn2\n \n=\n \nNormal\n(\n8.0\n,\n \n1.2\n)\n\n\nn3\n \n=\n \nNormal\n(\n0.0\n,\n \n2.5\n)\n\n\n\n# Use a mixture model to create a bimodal distribution\n\n\nM\n \n=\n \nMixtureModel\n([\nn1\n,\n \nn2\n,\n \nn3\n])\n\n\n\n# Sample the mixture model.\n\n\nsamples_empirical\n \n=\n \nrand\n(\nM\n,\n \nInt\n(\n1e4\n));\n\n\n\n\n\n\n\n\n\nIt is not obvious which distribution to fit to such data.\n\n\nA kernel density estimate, however, will always be a decent representation of the data, because it doesn't follow a specific distribution and adapts to the data values.\n\n\nTo create a kernel density estimate, simply call the \nUncertainValue(v::Vector{Number})\n constructor with a vector containing the sample:\n\n\n1\nuv\n \n=\n \nUncertainValue\n(\nsamples_empirical\n)\n\n\n\n\n\n\n\nThe plot below compares the empirical histogram (here represented as a density plot) with our kernel density estimate.\n\n\n1\n2\n3\n4\n5\n6\n7\nusing\n \nPlots\n,\n \nStatPlots\n,\n \nUncertainData\n\n\nuv\n \n=\n \nUncertainValue\n(\nsamples_empirical\n)\n\n\ndensity\n(\nmvals\n,\n \nlabel\n \n=\n \n10000 mixture model (M) samples\n)\n\n\ndensity!\n(\nrand\n(\nuv\n,\n \nInt\n(\n1e4\n)),\n\n    \nlabel\n \n=\n \n10000 samples from KDE estimate to M\n)\n\n\nxlabel!\n(\ndata value\n)\n\n\nylabel!\n(\nprobability density\n)\n\n\n\n\n\n\n\n\n\n\n\nConstructor\n\n\n#\n\n\nUncertainData.UncertainValues.UncertainValue\n \n \nMethod\n.\n\n\n1\n2\nUncertainValue\n(\ndata\n::\nVector\n{\nT\n}\n;\n \nkernel\n::\nType\n{\nD\n}\n \n=\n \nNormal\n,\n \nnpoints\n::\nInt\n=\n2048\n)\n\n    \nwhere\n \n{\nD\n \n:\n \nDistribution,\n \nT\n}\n\n\n\n\n\n\n\nConstruct an uncertain value by a kernel density estimate to \ndata\n.\n\n\nArguments:\n\n\n\n\nkernel\n: The kernel to use. Defaults to \nDistributions.Normal\n. Must be   a valid family from \nDistributions.jl\n.\n\n\nnpoints\n: The number of points to use for the kernel density estimation.   Fast Fourier transforms are used, so the number of points should be a power   of 2 (default = 2048).\n\n\n\n\nsource\n\n\n\n\nAdditional keyword arguments and examples\n\n\nIf the only argument to the \nUncertainValue\n constructor is a vector of values, the default behaviour is to represent the distribution by a kernel density estimate (KDE), i.e. \nUncertainValue(data)\n. Gaussian kernels are used by default. The syntax \nUncertainValue(UnivariateKDE, data)\n will also work if \nKernelDensity.jl\n is loaded.", 
            "title": "Kernel density estimates (KDE)"
        }, 
        {
            "location": "/uncertainvalues_kde/#examples", 
            "text": "Implicit KDE constructor   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 using   Distributions ,   UncertainData  # Create a normal distribution  d   =   Normal ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Use the implicit KDE constructor to create the uncertain value  uv   =   UncertainValue ( v :: Vector )     Explicit KDE constructor   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 using   Distributions ,   UncertainData ,   KernelDensity  # Create a normal distribution  d   =   Normal ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Use the explicit KDE constructor to create the uncertain value.  # This constructor follows the same convention as when fitting distributions  # to empirical data, so this is the recommended way to construct KDE estimates.  uv   =   UncertainValue ( UnivariateKDE ,   v :: Vector )     Changing the kernel   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 using   Distributions ,   UncertainData ,   KernelDensity  # Create a normal distribution  d   =   Normal ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Use the explicit KDE constructor to create the uncertain value, specifying  # that we want to use normal distributions as the kernel. The kernel can be  # any valid kernel from Distributions.jl, and the default is to use normal  # distributions.  uv   =   UncertainValue ( UnivariateKDE ,   v :: Vector ;   kernel   =   Normal )     Adjusting number of points   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 using   Distributions ,   UncertainData ,   KernelDensity  # Create a normal distribution  d   =   Normal ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Use the explicit KDE constructor to create the uncertain value, specifying  # the number of points we want to use for the kernel density estimate. Fast  # Fourier transforms are used behind the scenes, so the number of points  # should be a power of 2 (the default is 2048 points).  uv   =   UncertainValue ( UnivariateKDE ,   v :: Vector ;   npoints   =   1024 )", 
            "title": "Examples"
        }, 
        {
            "location": "/uncertainvalues_kde/#extended_example", 
            "text": "Let's create a bimodal distribution, then sample 10000 values from it.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 using   Distributions  n1   =   Normal ( - 3.0 ,   1.2 )  n2   =   Normal ( 8.0 ,   1.2 )  n3   =   Normal ( 0.0 ,   2.5 )  # Use a mixture model to create a bimodal distribution  M   =   MixtureModel ([ n1 ,   n2 ,   n3 ])  # Sample the mixture model.  samples_empirical   =   rand ( M ,   Int ( 1e4 ));     It is not obvious which distribution to fit to such data.  A kernel density estimate, however, will always be a decent representation of the data, because it doesn't follow a specific distribution and adapts to the data values.  To create a kernel density estimate, simply call the  UncertainValue(v::Vector{Number})  constructor with a vector containing the sample:  1 uv   =   UncertainValue ( samples_empirical )    The plot below compares the empirical histogram (here represented as a density plot) with our kernel density estimate.  1\n2\n3\n4\n5\n6\n7 using   Plots ,   StatPlots ,   UncertainData  uv   =   UncertainValue ( samples_empirical )  density ( mvals ,   label   =   10000 mixture model (M) samples )  density! ( rand ( uv ,   Int ( 1e4 )), \n     label   =   10000 samples from KDE estimate to M )  xlabel! ( data value )  ylabel! ( probability density )", 
            "title": "Extended example"
        }, 
        {
            "location": "/uncertainvalues_kde/#constructor", 
            "text": "#  UncertainData.UncertainValues.UncertainValue     Method .  1\n2 UncertainValue ( data :: Vector { T } ;   kernel :: Type { D }   =   Normal ,   npoints :: Int = 2048 ) \n     where   { D   :   Distribution,   T }    Construct an uncertain value by a kernel density estimate to  data .  Arguments:   kernel : The kernel to use. Defaults to  Distributions.Normal . Must be   a valid family from  Distributions.jl .  npoints : The number of points to use for the kernel density estimation.   Fast Fourier transforms are used, so the number of points should be a power   of 2 (default = 2048).   source", 
            "title": "Constructor"
        }, 
        {
            "location": "/uncertainvalues_kde/#additional_keyword_arguments_and_examples", 
            "text": "If the only argument to the  UncertainValue  constructor is a vector of values, the default behaviour is to represent the distribution by a kernel density estimate (KDE), i.e.  UncertainValue(data) . Gaussian kernels are used by default. The syntax  UncertainValue(UnivariateKDE, data)  will also work if  KernelDensity.jl  is loaded.", 
            "title": "Additional keyword arguments and examples"
        }, 
        {
            "location": "/uncertainvalues_theoreticaldistributions/", 
            "text": "It is common in the scientific literature to encounter uncertain data values which are reported as following a specific distribution. For example, an author report the mean and standard deviation of a value stated to follow a normal distribution. \nUncertainData\n makes it easy to represent such values!\n\n\n\n\nSupported distributions\n\n\nSupported distributions are \nUniform\n, \nNormal\n, \nGamma\n, \nBeta\n, \nBetaPrime\n, \nFrechet\n, \nBinomial\n, \nBetaBinomial\n (more distributions will be added in the future!).\n\n\n\n\nExamples\n\n\n\n\n\n\nUniform\n\n\n1\n2\n# Uncertain value generated by a uniform distribution on [-5.0, 5.1].\n\n\nuv\n \n=\n \nUncertainValue\n(\nUniform\n,\n \n-\n5.0\n,\n \n5.1\n)\n\n\n\n\n\n\n\n\nNormal\n\n\n1\n2\n3\n# Uncertain value generated by a normal distribution with parameters \u03bc = -2 and\n\n\n# \u03c3 = 0.5.\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n-\n2\n,\n \n0.5\n)\n\n\n\n\n\n\n\n\nGamma\n\n\n1\n2\n3\n# Uncertain value generated by a gamma distribution with parameters \u03b1 = 2.2\n\n\n# and \u03b8 = 3.\n\n\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n2.2\n,\n \n3\n)\n\n\n\n\n\n\n\n\nBeta\n\n\n1\n2\n3\n# Uncertain value generated by a beta distribution with parameters \u03b1 = 1.5\n\n\n# and \u03b2 = 3.5\n\n\nuv\n \n=\n \nUncertainValue\n(\nBeta\n,\n \n1.5\n,\n \n3.5\n)\n\n\n\n\n\n\n\n\nBetaPrime\n\n\n1\n2\n3\n# Uncertain value generated by a beta prime distribution with parameters \u03b1 = 1.7\n\n\n# and \u03b2 = 3.2\n\n\nuv\n \n=\n \nUncertainValue\n(\nBeta\n,\n \n1.7\n,\n \n3.2\n)\n\n\n\n\n\n\n\n\nFr\u00e9chet\n\n\n1\n2\n3\n# Uncertain value generated by a Fr\u00e9chet distribution with parameters \u03b1 = 2.1\n\n\n# and \u03b8 = 4\n\n\nuv\n \n=\n \nUncertainValue\n(\nBeta\n,\n \n2.1\n,\n \n4\n)\n\n\n\n\n\n\n\n\nBinomial\n\n\n1\n2\n3\n# Uncertain value generated by binomial distribution with n = 28 trials and\n\n\n# probability p = 0.2 of success in individual trials.\n\n\nuv\n \n=\n \nUncertainValue\n(\nBinomial\n,\n \n28\n,\n \n0.2\n)\n\n\n\n\n\n\n\n\nBetaBinomial\n\n\n1\n2\n3\n# Creates an uncertain value generated by a beta-binomial distribution with\n\n\n# n = 28 trials, and parameters \u03b1 = 1.5 and \u03b2 = 3.5.\n\n\nuv\n \n=\n \nUncertainValue\n(\nBetaBinomial\n,\n \n28\n,\n \n3.3\n,\n \n4.4\n)\n\n\n\n\n\n\n\n\n\n\nConstructors\n\n\nThere are two constructors that creates uncertain values represented by theoretical distributions. Parameters are provided to the constructor in the same order as for constructing the equivalent distributions in \nDistributions.jl\n.\n\n\nUncertain values represented by theoretical distributions may be constructed using the two-parameter or three-parameter constructors \nUncertainValue(d::Type{D}, a\n:Number, b\n:Number)\n or \nUncertainValue(d::Type{D}, a\n:Number, b\n:Number, c\n:Number)\n (see below).\n\n\n\n\nTwo-parameter distributions\n\n\n#\n\n\nUncertainData.UncertainValues.UncertainValue\n \n \nMethod\n.\n\n\n1\n2\nUncertainValue\n(\ndistribution\n::\nType\n{\nD\n}\n,\n \na\n::\nT1\n,\n \nb\n::\nT2\n;\n\n    \nkwargs\n...)\n \nwhere\n \n{\nT1\n:Number,\n \nT2\n \n:\n \nNumber,\n \nD\n:Distribution\n}\n\n\n\n\n\n\n\nConstructor for two-parameter distributions\n\n\nUncertainValue\ns are currently implemented for the following two-parameter distributions: \nUniform\n, \nNormal\n, \nBinomial\n, \nBeta\n, \nBetaPrime\n, \nGamma\n, and \nFrechet\n.\n\n\nArguments\n\n\n\n\na\n, \nb\n: Generic parameters whose meaning varies depending   on what \ndistribution\n is provided. See the list below.\n\n\ndistribution\n: A valid univariate distribution from \nDistributions.jl\n.\n\n\n\n\nPrecisely what  \na\n and \nb\n are depends on which distribution is provided.\n\n\n\n\nUncertainValue(Normal, \u03bc, \u03c3)\n returns an \nUncertainScalarNormallyDistributed\n instance.\n\n\nUncertainValue(Uniform, lower, upper)\n returns an \nUncertainScalarUniformlyDistributed\n instance.\n\n\nUncertainValue(Beta, \u03b1, \u03b2)\n returns an \nUncertainScalarBetaDistributed\n instance.\n\n\nUncertainValue(BetaPrime, \u03b1, \u03b2)\n returns an \nUncertainScalarBetaPrimeDistributed\n instance.\n\n\nUncertainValue(Gamma, \u03b1, \u03b8)\n returns an \nUncertainScalarGammaDistributed\n instance.\n\n\nUncertainValue(Frechet, \u03b1, \u03b8)\n returns an \nUncertainScalarFrechetDistributed\n instance.\n\n\nUncertainValue(Binomial, n, p)\n returns an \nUncertainScalarBinomialDistributed\n instance.\n\n\n\n\nKeyword arguments\n\n\n\n\nn\u03c3\n: If \ndistribution \n: Distributions.Normal\n, then how many standard   deviations away from \n\u03bc\n does \nlower\n and \nupper\n (i.e. both, because   they are the same distance away from \n\u03bc\n) represent?\n\n\ntolerance\n: A threshold determining how symmetric the uncertainties   must be in order to allow the construction of  Normal distribution   (\nupper - lower \n threshold\n is required).\n\n\ntrunc_lower\n: Lower truncation bound for distributions with infinite   support. Defaults to \n-Inf\n.\n\n\ntrunc_upper\n: Upper truncation bound for distributions with infinite   support. Defaults to \nInf\n.\n\n\n\n\nExamples\n\n\nNormal distribution\n\n\nNormal distributions are formed by using the constructor \nUncertainValue(\u03bc, \u03c3, Normal; kwargs...)\n. This gives a normal distribution with mean \u03bc and standard deviation \u03c3/n\u03c3 (n\u03c3 must be given as a keyword argument).\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# A normal distribution with mean = 2.3 and standard deviation 0.3.\n\n\nUncertainValue\n(\n2.3\n,\n \n0.3\n,\n \nNormal\n)\n\n\n\n# A normal distribution with mean 2.3 and standard deviation 0.3/2.\n\n\nUncertainValue\n(\n2.3\n,\n \n0.3\n,\n \nNormal\n,\n \nn\u03c3\n \n=\n \n2\n)\n\n\n\n# A normal distribution with mean 2.3 and standard deviation = 0.3,\n\n\ntruncated\n \nto\n \nthe\n \ninterval\n \n`[1, 3]`\n.\n\n\nUncertainValue\n(\n2.3\n,\n \n0.3\n,\n \nNormal\n,\n \ntrunc_lower\n \n=\n \n1.0\n,\n \ntrunc_upper\n \n=\n \n3.0\n)\n\n\n\n\n\n\n\nUniform distribution\n\n\nUniform distributions are formed using the \nUncertainValue(lower, upper, Uniform)\n constructor.\n\n\n1\n2\n#  A uniform distribution on `[2, 3]`\n\n\nUncertainValue\n(\n-\n2\n,\n \n3\n,\n \nUniform\n)\n\n\n\n\n\n\n\nsource\n\n\n\n\nThree-parameter distributions\n\n\n#\n\n\nUncertainData.UncertainValues.UncertainValue\n \n \nMethod\n.\n\n\n1\n2\nUncertainValue\n(\ndistribution\n::\nType\n{\nD\n}\n,\n \na\n::\nT1\n,\n \nb\n::\nT2\n,\n \nc\n::\nT3\n;\n\n    \nkwargs\n...)\n \nwhere\n \n{\nT1\n:Number,\n \nT2\n:Number,\n \nT3\n:Number,\n \nD\n:Distribution\n}\n\n\n\n\n\n\n\nConstructor for three-parameter distributions\n\n\nCurrently implemented distributions are \nBetaBinomial\n.\n\n\nArguments\n\n\n\n\na\n, \nb\n, \nc\n: Generic parameters whose meaning varies depending   on what \ndistribution\n is provided. See the list below.\n\n\ndistribution\n: A valid univariate distribution from \nDistributions.jl\n.\n\n\n\n\nPrecisely what \na\n, \nb\n and \nc\n are depends on which distribution is provided.\n\n\n\n\nUncertainValue(BetaBinomial, n, \u03b1, \u03b2)\n returns an \nUncertainScalarBetaBinomialDistributed\n instance.\n\n\n\n\nKeyword arguments\n\n\n\n\nn\u03c3\n: If \ndistribution \n: Distributions.Normal\n, then how many standard   deviations away from \n\u03bc\n does \nlower\n and \nupper\n (i.e. both, because   they are the same distance away from \n\u03bc\n) represent?\n\n\ntolerance\n: A threshold determining how symmetric the uncertainties   must be in order to allow the construction of  Normal distribution   (\nupper - lower \n threshold\n is required).\n\n\ntrunc_lower\n: Lower truncation bound for distributions with infinite   support. Defaults to \n-Inf\n.\n\n\ntrunc_upper\n: Upper truncation bound for distributions with infinite   support. Defaults to \nInf\n.\n\n\n\n\nExamples\n\n\nBetaBinomial distribution\n\n\nNormal distributions are formed by using the constructor \nUncertainValue(\u03bc, \u03c3, Normal; kwargs...)\n. This gives a normal distribution with mean \u03bc and standard deviation \u03c3/n\u03c3 (n\u03c3 must be given as a keyword argument).\n\n\n1\n2\n3\n# A beta binomial distribution with n = 100 trials and parameters \u03b1 = 2.3 and\n\n\n# \u03b2 = 5\n\n\nUncertainValue\n(\n100\n,\n \n2.3\n,\n \n5\n,\n \nBetaBinomial\n)\n\n\n\n\n\n\n\nsource", 
            "title": "Theoretical distributions"
        }, 
        {
            "location": "/uncertainvalues_theoreticaldistributions/#supported_distributions", 
            "text": "Supported distributions are  Uniform ,  Normal ,  Gamma ,  Beta ,  BetaPrime ,  Frechet ,  Binomial ,  BetaBinomial  (more distributions will be added in the future!).", 
            "title": "Supported distributions"
        }, 
        {
            "location": "/uncertainvalues_theoreticaldistributions/#examples", 
            "text": "Uniform  1\n2 # Uncertain value generated by a uniform distribution on [-5.0, 5.1].  uv   =   UncertainValue ( Uniform ,   - 5.0 ,   5.1 )     Normal  1\n2\n3 # Uncertain value generated by a normal distribution with parameters \u03bc = -2 and  # \u03c3 = 0.5.  uv   =   UncertainValue ( Normal ,   - 2 ,   0.5 )     Gamma  1\n2\n3 # Uncertain value generated by a gamma distribution with parameters \u03b1 = 2.2  # and \u03b8 = 3.  uv   =   UncertainValue ( Gamma ,   2.2 ,   3 )     Beta  1\n2\n3 # Uncertain value generated by a beta distribution with parameters \u03b1 = 1.5  # and \u03b2 = 3.5  uv   =   UncertainValue ( Beta ,   1.5 ,   3.5 )     BetaPrime  1\n2\n3 # Uncertain value generated by a beta prime distribution with parameters \u03b1 = 1.7  # and \u03b2 = 3.2  uv   =   UncertainValue ( Beta ,   1.7 ,   3.2 )     Fr\u00e9chet  1\n2\n3 # Uncertain value generated by a Fr\u00e9chet distribution with parameters \u03b1 = 2.1  # and \u03b8 = 4  uv   =   UncertainValue ( Beta ,   2.1 ,   4 )     Binomial  1\n2\n3 # Uncertain value generated by binomial distribution with n = 28 trials and  # probability p = 0.2 of success in individual trials.  uv   =   UncertainValue ( Binomial ,   28 ,   0.2 )     BetaBinomial  1\n2\n3 # Creates an uncertain value generated by a beta-binomial distribution with  # n = 28 trials, and parameters \u03b1 = 1.5 and \u03b2 = 3.5.  uv   =   UncertainValue ( BetaBinomial ,   28 ,   3.3 ,   4.4 )", 
            "title": "Examples"
        }, 
        {
            "location": "/uncertainvalues_theoreticaldistributions/#constructors", 
            "text": "There are two constructors that creates uncertain values represented by theoretical distributions. Parameters are provided to the constructor in the same order as for constructing the equivalent distributions in  Distributions.jl .  Uncertain values represented by theoretical distributions may be constructed using the two-parameter or three-parameter constructors  UncertainValue(d::Type{D}, a :Number, b :Number)  or  UncertainValue(d::Type{D}, a :Number, b :Number, c :Number)  (see below).", 
            "title": "Constructors"
        }, 
        {
            "location": "/uncertainvalues_theoreticaldistributions/#two-parameter_distributions", 
            "text": "#  UncertainData.UncertainValues.UncertainValue     Method .  1\n2 UncertainValue ( distribution :: Type { D } ,   a :: T1 ,   b :: T2 ; \n     kwargs ...)   where   { T1 :Number,   T2   :   Number,   D :Distribution }    Constructor for two-parameter distributions  UncertainValue s are currently implemented for the following two-parameter distributions:  Uniform ,  Normal ,  Binomial ,  Beta ,  BetaPrime ,  Gamma , and  Frechet .  Arguments   a ,  b : Generic parameters whose meaning varies depending   on what  distribution  is provided. See the list below.  distribution : A valid univariate distribution from  Distributions.jl .   Precisely what   a  and  b  are depends on which distribution is provided.   UncertainValue(Normal, \u03bc, \u03c3)  returns an  UncertainScalarNormallyDistributed  instance.  UncertainValue(Uniform, lower, upper)  returns an  UncertainScalarUniformlyDistributed  instance.  UncertainValue(Beta, \u03b1, \u03b2)  returns an  UncertainScalarBetaDistributed  instance.  UncertainValue(BetaPrime, \u03b1, \u03b2)  returns an  UncertainScalarBetaPrimeDistributed  instance.  UncertainValue(Gamma, \u03b1, \u03b8)  returns an  UncertainScalarGammaDistributed  instance.  UncertainValue(Frechet, \u03b1, \u03b8)  returns an  UncertainScalarFrechetDistributed  instance.  UncertainValue(Binomial, n, p)  returns an  UncertainScalarBinomialDistributed  instance.   Keyword arguments   n\u03c3 : If  distribution  : Distributions.Normal , then how many standard   deviations away from  \u03bc  does  lower  and  upper  (i.e. both, because   they are the same distance away from  \u03bc ) represent?  tolerance : A threshold determining how symmetric the uncertainties   must be in order to allow the construction of  Normal distribution   ( upper - lower   threshold  is required).  trunc_lower : Lower truncation bound for distributions with infinite   support. Defaults to  -Inf .  trunc_upper : Upper truncation bound for distributions with infinite   support. Defaults to  Inf .   Examples  Normal distribution  Normal distributions are formed by using the constructor  UncertainValue(\u03bc, \u03c3, Normal; kwargs...) . This gives a normal distribution with mean \u03bc and standard deviation \u03c3/n\u03c3 (n\u03c3 must be given as a keyword argument).  1\n2\n3\n4\n5\n6\n7\n8\n9 # A normal distribution with mean = 2.3 and standard deviation 0.3.  UncertainValue ( 2.3 ,   0.3 ,   Normal )  # A normal distribution with mean 2.3 and standard deviation 0.3/2.  UncertainValue ( 2.3 ,   0.3 ,   Normal ,   n\u03c3   =   2 )  # A normal distribution with mean 2.3 and standard deviation = 0.3,  truncated   to   the   interval   `[1, 3]` .  UncertainValue ( 2.3 ,   0.3 ,   Normal ,   trunc_lower   =   1.0 ,   trunc_upper   =   3.0 )    Uniform distribution  Uniform distributions are formed using the  UncertainValue(lower, upper, Uniform)  constructor.  1\n2 #  A uniform distribution on `[2, 3]`  UncertainValue ( - 2 ,   3 ,   Uniform )    source", 
            "title": "Two-parameter distributions"
        }, 
        {
            "location": "/uncertainvalues_theoreticaldistributions/#three-parameter_distributions", 
            "text": "#  UncertainData.UncertainValues.UncertainValue     Method .  1\n2 UncertainValue ( distribution :: Type { D } ,   a :: T1 ,   b :: T2 ,   c :: T3 ; \n     kwargs ...)   where   { T1 :Number,   T2 :Number,   T3 :Number,   D :Distribution }    Constructor for three-parameter distributions  Currently implemented distributions are  BetaBinomial .  Arguments   a ,  b ,  c : Generic parameters whose meaning varies depending   on what  distribution  is provided. See the list below.  distribution : A valid univariate distribution from  Distributions.jl .   Precisely what  a ,  b  and  c  are depends on which distribution is provided.   UncertainValue(BetaBinomial, n, \u03b1, \u03b2)  returns an  UncertainScalarBetaBinomialDistributed  instance.   Keyword arguments   n\u03c3 : If  distribution  : Distributions.Normal , then how many standard   deviations away from  \u03bc  does  lower  and  upper  (i.e. both, because   they are the same distance away from  \u03bc ) represent?  tolerance : A threshold determining how symmetric the uncertainties   must be in order to allow the construction of  Normal distribution   ( upper - lower   threshold  is required).  trunc_lower : Lower truncation bound for distributions with infinite   support. Defaults to  -Inf .  trunc_upper : Upper truncation bound for distributions with infinite   support. Defaults to  Inf .   Examples  BetaBinomial distribution  Normal distributions are formed by using the constructor  UncertainValue(\u03bc, \u03c3, Normal; kwargs...) . This gives a normal distribution with mean \u03bc and standard deviation \u03c3/n\u03c3 (n\u03c3 must be given as a keyword argument).  1\n2\n3 # A beta binomial distribution with n = 100 trials and parameters \u03b1 = 2.3 and  # \u03b2 = 5  UncertainValue ( 100 ,   2.3 ,   5 ,   BetaBinomial )    source", 
            "title": "Three-parameter distributions"
        }, 
        {
            "location": "/uncertainvalues_fitted/", 
            "text": "For data values with histograms close to some known distribution, the user may choose to represent the data by fitting a theoretical distribution to the values. This will only work well if the histogram closely resembles a theoretical distribution.\n\n\n\n\nExamples\n\n\n\n\n\n\nUniform\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nUniform\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Define an uncertain value by fitting a uniform distribution to the sample.\n\n\nuv\n \n=\n \nUncertainValue\n(\nUniform\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\nNormal\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Create a normal distribution\n\n\nd\n \n=\n \nNormal\n()\n\n\n\n# Draw a 1000-point sample from the distribution.\n\n\nsome_sample\n \n=\n \nrand\n(\nd\n,\n \n1000\n)\n\n\n\n# Represent the uncertain value by a fitted normal distribution.\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\nGamma\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate 1000 values from a gamma distribution with parameters \u03b1 = 2.1,\n\n\n# \u03b8 = 5.2.\n\n\nsome_sample\n \n=\n \nrand\n(\nGamma\n(\n2.1\n,\n \n5.2\n),\n \n1000\n)\n\n\n\n# Represent the uncertain value by a fitted gamma distribution.\n\n\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\n\nIn these examples we're trying to fit the same distribution to our sample as the distribution from which we draw the sample. Thus, we will get good fits. In real applications, make sure to always visually investigate the histogram of your data!\n\n\n\n\nBeware: fitting distributions may lead to nonsensical results!\n\n\nIn a less contrived example, we may try to fit a beta distribution to a sample generated from a gamma distribution.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nusing\n \nDistributions\n,\n \nUncertainData\n\n\n\n# Generate 1000 values from a gamma distribution with parameters \u03b1 = 2.1,\n\n\n# \u03b8 = 5.2.\n\n\nsome_sample\n \n=\n \nrand\n(\nGamma\n(\n2.1\n,\n \n5.2\n),\n \n1000\n)\n\n\n\n# Represent the uncertain value by a fitted beta distribution.\n\n\nuv\n \n=\n \nUncertainValue\n(\nBeta\n,\n \nsome_sample\n)\n\n\n\n\n\n\n\nThis is obviously not a good idea. Always visualise your distribution before deciding on which distribution to fit! You won't get any error messages if you try to fit a distribution that does not match your data.\n\n\nIf the data do not follow an obvious theoretical distribution, it is better to use \nkernel density estimation\n to define the uncertain value.\n\n\n\n\nConstructor\n\n\n#\n\n\nUncertainData.UncertainValues.UncertainValue\n \n \nMethod\n.\n\n\n1\n2\nUncertainValue(d::Type{D},\n    empiricaldata::Vector{T}) where {D\n:Distribution, T}\n\n\n\n\n\n\nConstructor for empirical distributions.\n\n\nFit a distribution of type \nd\n to the data and use that as the representation of the empirical distribution. Calls \nDistributions.fit\n behind the scenes.\n\n\nArguments\n\n\n\n\nempiricaldata\n: The data for which to fit the \ndistribution\n.\n\n\ndistribution\n: A valid univariate distribution from \nDistributions.jl\n.\n\n\n\n\nsource", 
            "title": "Fit theoretical distributions"
        }, 
        {
            "location": "/uncertainvalues_fitted/#examples", 
            "text": "Uniform   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 using   Distributions ,   UncertainData  # Create a normal distribution  d   =   Uniform ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Define an uncertain value by fitting a uniform distribution to the sample.  uv   =   UncertainValue ( Uniform ,   some_sample )     Normal   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 using   Distributions ,   UncertainData  # Create a normal distribution  d   =   Normal ()  # Draw a 1000-point sample from the distribution.  some_sample   =   rand ( d ,   1000 )  # Represent the uncertain value by a fitted normal distribution.  uv   =   UncertainValue ( Normal ,   some_sample )     Gamma  1\n2\n3\n4\n5\n6\n7\n8 using   Distributions ,   UncertainData  # Generate 1000 values from a gamma distribution with parameters \u03b1 = 2.1,  # \u03b8 = 5.2.  some_sample   =   rand ( Gamma ( 2.1 ,   5.2 ),   1000 )  # Represent the uncertain value by a fitted gamma distribution.  uv   =   UncertainValue ( Gamma ,   some_sample )     In these examples we're trying to fit the same distribution to our sample as the distribution from which we draw the sample. Thus, we will get good fits. In real applications, make sure to always visually investigate the histogram of your data!", 
            "title": "Examples"
        }, 
        {
            "location": "/uncertainvalues_fitted/#beware_fitting_distributions_may_lead_to_nonsensical_results", 
            "text": "In a less contrived example, we may try to fit a beta distribution to a sample generated from a gamma distribution.  1\n2\n3\n4\n5\n6\n7\n8 using   Distributions ,   UncertainData  # Generate 1000 values from a gamma distribution with parameters \u03b1 = 2.1,  # \u03b8 = 5.2.  some_sample   =   rand ( Gamma ( 2.1 ,   5.2 ),   1000 )  # Represent the uncertain value by a fitted beta distribution.  uv   =   UncertainValue ( Beta ,   some_sample )    This is obviously not a good idea. Always visualise your distribution before deciding on which distribution to fit! You won't get any error messages if you try to fit a distribution that does not match your data.  If the data do not follow an obvious theoretical distribution, it is better to use  kernel density estimation  to define the uncertain value.", 
            "title": "Beware: fitting distributions may lead to nonsensical results!"
        }, 
        {
            "location": "/uncertainvalues_fitted/#constructor", 
            "text": "#  UncertainData.UncertainValues.UncertainValue     Method .  1\n2 UncertainValue(d::Type{D},\n    empiricaldata::Vector{T}) where {D :Distribution, T}   Constructor for empirical distributions.  Fit a distribution of type  d  to the data and use that as the representation of the empirical distribution. Calls  Distributions.fit  behind the scenes.  Arguments   empiricaldata : The data for which to fit the  distribution .  distribution : A valid univariate distribution from  Distributions.jl .   source", 
            "title": "Constructor"
        }, 
        {
            "location": "/ensemble_statistics/", 
            "text": "Uncertain statistics\n\n\n\n\nCore statistics\n\n\nThis package implements most of the statistical algorithms in \nStatsBase\n for uncertain values and uncertain datasets.\n\n\nThe syntax for calling the algorithms is the same as in \nStatsBase\n, but the functions here accept an additional positional argument \nn\n, which controls how many times the uncertain values are resampled to compute the statistics.\n\n\n\n\nStatistics of single uncertain values\n\n\n#\n\n\nStatistics.mean\n \n \nMethod\n.\n\n\n1\nmean(uv::AbstractUncertainValue, n::Int)\n\n\n\n\n\n\nCompute the mean of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n#\n\n\nStatistics.median\n \n \nMethod\n.\n\n\n1\nmedian(uv::AbstractUncertainValue, n::Int)\n\n\n\n\n\n\nCompute the median of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n#\n\n\nStatistics.middle\n \n \nMethod\n.\n\n\n1\nmiddle(uv::AbstractUncertainValue, n::Int)\n\n\n\n\n\n\nCompute the middle of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n#\n\n\nStatistics.std\n \n \nMethod\n.\n\n\n1\nstd(uv::AbstractUncertainValue, n::Int)\n\n\n\n\n\n\nCompute the standard deviation of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n#\n\n\nStatistics.var\n \n \nMethod\n.\n\n\n1\nvariance(uv::AbstractUncertainValue, n::Int)\n\n\n\n\n\n\nCompute the variance of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n#\n\n\nStatistics.quantile\n \n \nMethod\n.\n\n\n1\nquantile(uv::AbstractUncertainValue, q, n::Int)\n\n\n\n\n\n\nCompute the quantile(s) \nq\n of an uncertain value over an \nn\n-draw sample of it.\n\n\nsource\n\n\n\n\nStatistics on datasets of uncertain values\n\n\nThe following statistics are available for uncertain datasets (collections of uncertain values).\n\n\n#\n\n\nStatistics.mean\n \n \nMethod\n.\n\n\n1\nmean(d::UncertainDataset, n::Int)\n\n\n\n\n\n\nComputes the element-wise mean of a dataset of uncertain values. Takes the mean of an \nn\n-draw sample for each element.\n\n\nsource\n\n\n#\n\n\nStatistics.median\n \n \nMethod\n.\n\n\n1\nmedian(d::UncertainDataset, n::Int)\n\n\n\n\n\n\nComputes the element-wise median of a dataset of uncertain values. Takes the median of an \nn\n-draw sample for each element.\n\n\nsource\n\n\n#\n\n\nStatistics.middle\n \n \nMethod\n.\n\n\n1\nmiddle(d::UncertainDataset, n::Int)\n\n\n\n\n\n\nCompute the middle of \nn\n realisations of an \nUncertainDataset\n.\n\n\nsource\n\n\n#\n\n\nStatistics.std\n \n \nMethod\n.\n\n\n1\nstd\n(\nd\n::\nUncertainDataset\n,\n \nn\n::\nInt\n;\n \nkwargs\n...)\n\n\n\n\n\n\n\nComputes the element-wise standard deviation of a dataset of uncertain values. Takes the standard deviation of an \nn\n-draw sample for each element.\n\n\nsource\n\n\n#\n\n\nStatistics.var\n \n \nMethod\n.\n\n\n1\nvar\n(\nd\n::\nUncertainDataset\n,\n \nn\n::\nInt\n;\n \nkwargs\n...)\n\n\n\n\n\n\n\nComputes the element-wise sample variance of a dataset of uncertain values. Takes the sample variance of an \nn\n-draw sample for each element.\n\n\nsource\n\n\n#\n\n\nStatistics.quantile\n \n \nMethod\n.\n\n\n1\nquantile\n(\nd\n::\nUncertainDataset\n,\n \np\n,\n \nn\n::\nInt\n;\n \nkwargs\n...)\n\n\n\n\n\n\n\nCompute element-wise quantile(s) \np\nof a dataset consisting of uncertain values. Takes the quantiles of an \nn\n-draw sample for each element.\n\n\nsource\n\n\n#\n\n\nStatistics.cov\n \n \nMethod\n.\n\n\n1\ncov\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n \nn\n::\nInt\n;\n \nkwargs\n...)\n\n\n\n\n\n\n\nCompute the covariance between two \nUncertainDataset\ns by realising both datasets \nn\n times.\n\n\nsource\n\n\n#\n\n\nStatistics.cor\n \n \nMethod\n.\n\n\n1\ncor\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n \nn\n::\nInt\n;\n \nkwargs\n...)\n\n\n\n\n\n\n\nCompute the Pearson correlation between two \nUncertainDataset\ns by realising both datasets \nn\n times.\n\n\nsource", 
            "title": "Core statistics"
        }, 
        {
            "location": "/ensemble_statistics/#uncertain_statistics", 
            "text": "", 
            "title": "Uncertain statistics"
        }, 
        {
            "location": "/ensemble_statistics/#core_statistics", 
            "text": "This package implements most of the statistical algorithms in  StatsBase  for uncertain values and uncertain datasets.  The syntax for calling the algorithms is the same as in  StatsBase , but the functions here accept an additional positional argument  n , which controls how many times the uncertain values are resampled to compute the statistics.", 
            "title": "Core statistics"
        }, 
        {
            "location": "/ensemble_statistics/#statistics_of_single_uncertain_values", 
            "text": "#  Statistics.mean     Method .  1 mean(uv::AbstractUncertainValue, n::Int)   Compute the mean of an uncertain value over an  n -draw sample of it.  source  #  Statistics.median     Method .  1 median(uv::AbstractUncertainValue, n::Int)   Compute the median of an uncertain value over an  n -draw sample of it.  source  #  Statistics.middle     Method .  1 middle(uv::AbstractUncertainValue, n::Int)   Compute the middle of an uncertain value over an  n -draw sample of it.  source  #  Statistics.std     Method .  1 std(uv::AbstractUncertainValue, n::Int)   Compute the standard deviation of an uncertain value over an  n -draw sample of it.  source  #  Statistics.var     Method .  1 variance(uv::AbstractUncertainValue, n::Int)   Compute the variance of an uncertain value over an  n -draw sample of it.  source  #  Statistics.quantile     Method .  1 quantile(uv::AbstractUncertainValue, q, n::Int)   Compute the quantile(s)  q  of an uncertain value over an  n -draw sample of it.  source", 
            "title": "Statistics of single uncertain values"
        }, 
        {
            "location": "/ensemble_statistics/#statistics_on_datasets_of_uncertain_values", 
            "text": "The following statistics are available for uncertain datasets (collections of uncertain values).  #  Statistics.mean     Method .  1 mean(d::UncertainDataset, n::Int)   Computes the element-wise mean of a dataset of uncertain values. Takes the mean of an  n -draw sample for each element.  source  #  Statistics.median     Method .  1 median(d::UncertainDataset, n::Int)   Computes the element-wise median of a dataset of uncertain values. Takes the median of an  n -draw sample for each element.  source  #  Statistics.middle     Method .  1 middle(d::UncertainDataset, n::Int)   Compute the middle of  n  realisations of an  UncertainDataset .  source  #  Statistics.std     Method .  1 std ( d :: UncertainDataset ,   n :: Int ;   kwargs ...)    Computes the element-wise standard deviation of a dataset of uncertain values. Takes the standard deviation of an  n -draw sample for each element.  source  #  Statistics.var     Method .  1 var ( d :: UncertainDataset ,   n :: Int ;   kwargs ...)    Computes the element-wise sample variance of a dataset of uncertain values. Takes the sample variance of an  n -draw sample for each element.  source  #  Statistics.quantile     Method .  1 quantile ( d :: UncertainDataset ,   p ,   n :: Int ;   kwargs ...)    Compute element-wise quantile(s)  p of a dataset consisting of uncertain values. Takes the quantiles of an  n -draw sample for each element.  source  #  Statistics.cov     Method .  1 cov ( d1 :: UncertainDataset ,   d2 :: UncertainDataset ,   n :: Int ;   kwargs ...)    Compute the covariance between two  UncertainDataset s by realising both datasets  n  times.  source  #  Statistics.cor     Method .  1 cor ( d1 :: UncertainDataset ,   d2 :: UncertainDataset ,   n :: Int ;   kwargs ...)    Compute the Pearson correlation between two  UncertainDataset s by realising both datasets  n  times.  source", 
            "title": "Statistics on datasets of uncertain values"
        }, 
        {
            "location": "/hypothesistests/hypothesis_tests_overview/", 
            "text": "Hypothesis tests\n\n\nIn addition to providing ensemble computation of basic statistic measures, this package also wraps various hypothesis tests from \nHypothesisTests.jl\n. This allows us to perform hypothesis testing on ensemble realisations of the data.\n\n\n\n\nImplemented hypothesis tests\n\n\nThe following hypothesis tests are implemented for uncertain data types.\n\n\n\n\nOne sample t-test\n.\n\n\nEqual variance t-test\n.\n\n\nUnequal variance t-test\n.\n\n\nExact Kolmogorov-Smirnov test\n.\n\n\nApproximate two-sample Kolmogorov-Smirnov test\n.\n\n\nOne-sample Anderson\u2013Darling test\n.\n\n\nJarque-Bera test\n.\n\n\n\n\n\n\nTerminology\n\n\nPooled statistics\n are computed by sampling all uncertain values comprising the dataset n times, pooling the values together and treating them as one variable, then computing the statistic.\n\n\nElement-wise statistics\n are computed by sampling each uncertain value n times, keeping the data generated from each uncertain value separate. The statistics are the computed separately for each sample.", 
            "title": "Overview"
        }, 
        {
            "location": "/hypothesistests/hypothesis_tests_overview/#hypothesis_tests", 
            "text": "In addition to providing ensemble computation of basic statistic measures, this package also wraps various hypothesis tests from  HypothesisTests.jl . This allows us to perform hypothesis testing on ensemble realisations of the data.", 
            "title": "Hypothesis tests"
        }, 
        {
            "location": "/hypothesistests/hypothesis_tests_overview/#implemented_hypothesis_tests", 
            "text": "The following hypothesis tests are implemented for uncertain data types.   One sample t-test .  Equal variance t-test .  Unequal variance t-test .  Exact Kolmogorov-Smirnov test .  Approximate two-sample Kolmogorov-Smirnov test .  One-sample Anderson\u2013Darling test .  Jarque-Bera test .", 
            "title": "Implemented hypothesis tests"
        }, 
        {
            "location": "/hypothesistests/hypothesis_tests_overview/#terminology", 
            "text": "Pooled statistics  are computed by sampling all uncertain values comprising the dataset n times, pooling the values together and treating them as one variable, then computing the statistic.  Element-wise statistics  are computed by sampling each uncertain value n times, keeping the data generated from each uncertain value separate. The statistics are the computed separately for each sample.", 
            "title": "Terminology"
        }, 
        {
            "location": "/hypothesistests/one_sample_t_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests.OneSampleTTest\n \n \nType\n.\n\n\n1\n2\nOneSampleTTest\n(\nd\n::\nAbstractUncertainValue\n,\n \nn\n::\nInt\n \n=\n \n1000\n;\n\n    \n\u03bc\n0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nOneSampleTTest\n\n\n\n\n\n\n\nPerform a one sample t-test of the null hypothesis that the uncertain value has a distribution with mean \n\u03bc0\n against the alternative hypothesis that its distribution does not have mean \n\u03bc0\n. \nn\n indicates the number of draws during resampling.\n\n\nsource\n\n\nExample:\n\n\n1\n2\n3\n4\n5\n6\n# Normally distributed uncertain observation with mean = 2.1\n\n\nuv\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n2.1\n,\n \n0.2\n)\n\n\n\n# Perform a one-sample t-test to test the null hypothesis that\n\n\n# the sample comes from a distribution with mean \u03bc0\n\n\nOneSampleTTest\n(\nuv\n,\n \n1000\n,\n \n\u03bc0\n \n=\n \n2.1\n)\n\n\n\n\n\n\n\nWhich gives the following output:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Which results in\n\n\nOne\n \nsample\n \nt\n-\ntest\n\n\n-----------------\n\n\nPopulation\n \ndetails\n:\n\n    \nparameter\n \nof\n \ninterest\n:\n   \nMean\n\n    \nvalue\n \nunder\n \nh_0\n:\n         \n2.1\n\n    \npoint\n \nestimate\n:\n          \n2.1031909275381566\n\n    \n95\n%\n \nconfidence\n \ninterval\n:\n \n(\n2.091\n,\n \n2.1154\n)\n\n\n\nTest\n \nsummary\n:\n\n    \noutcome\n \nwith\n \n95\n%\n \nconfidence\n:\n \nfail\n \nto\n \nreject\n \nh_0\n\n    \ntwo\n-\nsided\n \np\n-\nvalue\n:\n           \n0.6089\n\n\n\nDetails\n:\n\n    \nnumber\n \nof\n \nobservations\n:\n   \n1000\n\n    \nt\n-\nstatistic\n:\n              \n0.5117722099885472\n\n    \ndegrees\n \nof\n \nfreedom\n:\n       \n999\n\n    \nempirical\n \nstandard\n \nerror\n:\n \n0.00623505433839\n\n\n\n\n\n\n\nThus, we cannot reject the null-hypothesis that the sample comes from a distribution with mean = 2.1. Therefore, we accept the alternative hypothesis that our sample \ndoes\n in fact come from such a distribution. This is of course true, because we defined the uncertain value as a normal distribution with mean 2.1.\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData.UncertainStatistics.OneSampleTTestPooled\n \n \nFunction\n.\n\n\n1\n2\n3\nOneSampleTTestPooled\n(\nd1\n::\nUncertainDataset\n,\n\n    \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc\n0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nOneSampleTTest\n\n\n\n\n\n\n\nFirst, sample \nn\n draws of each uncertain value in each dataset, pooling the draws from the elements of \nd1\n and the draws from the elements of \nd2\n separately. Then, perform a paired sample t-test of the null hypothesis that the differences between pairs of uncertain values in \nd1\n and \nd2\n come from a distribution with mean \n\u03bc0\n against the alternative hypothesis that the distribution does not have mean \n\u03bc0\n.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData.UncertainStatistics.OneSampleTTestElementWise\n \n \nFunction\n.\n\n\n1\n2\n3\nOneSampleTTestElementWise\n(\nd1\n::\nUncertainDataset\n,\n\n    \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc\n0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nVector\n{\nOneSampleTTest\n}\n\n\n\n\n\n\n\nPerform a one sample t-test of the null hypothesis that the uncertain value has a distribution with mean \n\u03bc0\n against the alternative hypothesis that its distribution does not have mean \n\u03bc0\n for uncertain value in \nd\n.\n\n\nn\n indicates the number of draws during resampling.\n\n\nsource", 
            "title": "One sample t-test"
        }, 
        {
            "location": "/hypothesistests/one_sample_t_test/#regular_test", 
            "text": "#  HypothesisTests.OneSampleTTest     Type .  1\n2 OneSampleTTest ( d :: AbstractUncertainValue ,   n :: Int   =   1000 ; \n     \u03bc 0 :: Real   =   0 )   -   OneSampleTTest    Perform a one sample t-test of the null hypothesis that the uncertain value has a distribution with mean  \u03bc0  against the alternative hypothesis that its distribution does not have mean  \u03bc0 .  n  indicates the number of draws during resampling.  source  Example:  1\n2\n3\n4\n5\n6 # Normally distributed uncertain observation with mean = 2.1  uv   =   UncertainValue ( Normal ,   2.1 ,   0.2 )  # Perform a one-sample t-test to test the null hypothesis that  # the sample comes from a distribution with mean \u03bc0  OneSampleTTest ( uv ,   1000 ,   \u03bc0   =   2.1 )    Which gives the following output:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # Which results in  One   sample   t - test  -----------------  Population   details : \n     parameter   of   interest :     Mean \n     value   under   h_0 :           2.1 \n     point   estimate :            2.1031909275381566 \n     95 %   confidence   interval :   ( 2.091 ,   2.1154 )  Test   summary : \n     outcome   with   95 %   confidence :   fail   to   reject   h_0 \n     two - sided   p - value :             0.6089  Details : \n     number   of   observations :     1000 \n     t - statistic :                0.5117722099885472 \n     degrees   of   freedom :         999 \n     empirical   standard   error :   0.00623505433839    Thus, we cannot reject the null-hypothesis that the sample comes from a distribution with mean = 2.1. Therefore, we accept the alternative hypothesis that our sample  does  in fact come from such a distribution. This is of course true, because we defined the uncertain value as a normal distribution with mean 2.1.", 
            "title": "Regular test"
        }, 
        {
            "location": "/hypothesistests/one_sample_t_test/#pooled_test", 
            "text": "#  UncertainData.UncertainStatistics.OneSampleTTestPooled     Function .  1\n2\n3 OneSampleTTestPooled ( d1 :: UncertainDataset , \n     d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc 0 :: Real   =   0 )   -   OneSampleTTest    First, sample  n  draws of each uncertain value in each dataset, pooling the draws from the elements of  d1  and the draws from the elements of  d2  separately. Then, perform a paired sample t-test of the null hypothesis that the differences between pairs of uncertain values in  d1  and  d2  come from a distribution with mean  \u03bc0  against the alternative hypothesis that the distribution does not have mean  \u03bc0 .  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/hypothesistests/one_sample_t_test/#element-wise_test", 
            "text": "#  UncertainData.UncertainStatistics.OneSampleTTestElementWise     Function .  1\n2\n3 OneSampleTTestElementWise ( d1 :: UncertainDataset , \n     d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc 0 :: Real   =   0 )   -   Vector { OneSampleTTest }    Perform a one sample t-test of the null hypothesis that the uncertain value has a distribution with mean  \u03bc0  against the alternative hypothesis that its distribution does not have mean  \u03bc0  for uncertain value in  d .  n  indicates the number of draws during resampling.  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/hypothesistests/equal_variance_t_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests.EqualVarianceTTest\n \n \nType\n.\n\n\n1\n2\nEqualVarianceTTest\n(\nd1\n::\nAbstractUncertainValue\n,\n \nd2\n::\nAbstractUncertainValue\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc\n0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nEqualVarianceTTest\n\n\n\n\n\n\n\nConsider two samples \ns1\n and \ns2\n, each consisting of \nn\n random draws from the distributions furnishing \nd1\n and \nd2\n, respectively.\n\n\nThis function performs a two-sample t-test of the null hypothesis that \ns1\n and \ns2\n come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.\n\n\nsource\n\n\nExample\n\n\nLet's create two uncertain values furnished by distributions of different types. We'll perform the equal variance t-test to check if there is support for the null-hypothesis that the distributions furnishing the uncertain values come from distributions with equal means and variances.\n\n\nWe expect the test to reject this null-hypothesis, because we've created two very different distributions.\n\n\n1\n2\n3\n4\n5\nuv1\n \n=\n \nUncertainValue\n(\nNormal\n,\n \n1.2\n,\n \n0.3\n)\n\n\nuv2\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n2\n,\n \n3\n)\n\n\n\n# EqualVarianceTTest on 1000 draws for each variable\n\n\nEqualVarianceTTest\n(\nuv1\n,\n \nuv2\n,\n \n1000\n)\n\n\n\n\n\n\n\nThe output is:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nTwo\n \nsample\n \nt\n-\ntest\n \n(\nequal\n \nvariance\n)\n\n\n----------------------------------\n\n\nPopulation\n \ndetails\n:\n\n    \nparameter\n \nof\n \ninterest\n:\n   \nMean\n \ndifference\n\n    \nvalue\n \nunder\n \nh_0\n:\n         \n0\n\n    \npoint\n \nestimate\n:\n          \n-\n4.782470406651697\n\n    \n95\n%\n \nconfidence\n \ninterval\n:\n \n(\n-\n5.0428\n,\n \n-\n4.5222\n)\n\n\n\nTest\n \nsummary\n:\n\n    \noutcome\n \nwith\n \n95\n%\n \nconfidence\n:\n \nreject\n \nh_0\n\n    \ntwo\n-\nsided\n \np\n-\nvalue\n:\n           \n1e-99\n\n\n\nDetails\n:\n\n    \nnumber\n \nof\n \nobservations\n:\n   \n[\n1000\n,\n1000\n]\n\n    \nt\n-\nstatistic\n:\n              \n-\n36.03293014520585\n\n    \ndegrees\n \nof\n \nfreedom\n:\n       \n1998\n\n    \nempirical\n \nstandard\n \nerror\n:\n \n0.1327249931487462\n\n\n\n\n\n\n\nThe test rejects the null-hypothesis, so we accept the alternative hypothesis that the samples come from distributions with different means and variances.\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData.UncertainStatistics.EqualVarianceTTestPooled\n \n \nFunction\n.\n\n\n1\n2\nEqualVarianceTTestPooled\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc\n0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nEqualVarianceTTest\n\n\n\n\n\n\n\nConsider two samples \ns1[i]\n and \ns2[i]\n, each consisting of \nn\n random draws from the distributions furnishing the uncertain values \nd1[i]\n and \nd2[i]\n, respectively. Gather all \ns1[i]\n in a pooled sample \nS1\n, and all \ns2[i]\n in a pooled sample \nS2\n.\n\n\nPerform a two-sample t-test of the null hypothesis that \nS1\n and \nS2\n come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData.UncertainStatistics.EqualVarianceTTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nEqualVarianceTTestElementWise\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc\n0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nVector\n{\nEqualVarianceTTest\n}\n\n\n\n\n\n\n\nConsider two samples \ns1[i]\n and \ns2[i]\n, each consisting of \nn\n random draws from the distributions furnishing the uncertain values \nd1[i]\n and \nd2[i]\n, respectively. This function performs an elementwise \nEqualVarianceTTest\n on the pairs \n(s1[i], s2[i])\n. Specifically:\n\n\nPerforms an pairwise two-sample t-test of the null hypothesis that \ns1[i]\n and \ns2[i]\n come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.\n\n\nsource", 
            "title": "Equal variance t-test"
        }, 
        {
            "location": "/hypothesistests/equal_variance_t_test/#regular_test", 
            "text": "#  HypothesisTests.EqualVarianceTTest     Type .  1\n2 EqualVarianceTTest ( d1 :: AbstractUncertainValue ,   d2 :: AbstractUncertainValue , \n     n :: Int   =   1000 ;   \u03bc 0 :: Real   =   0 )   -   EqualVarianceTTest    Consider two samples  s1  and  s2 , each consisting of  n  random draws from the distributions furnishing  d1  and  d2 , respectively.  This function performs a two-sample t-test of the null hypothesis that  s1  and  s2  come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.  source  Example  Let's create two uncertain values furnished by distributions of different types. We'll perform the equal variance t-test to check if there is support for the null-hypothesis that the distributions furnishing the uncertain values come from distributions with equal means and variances.  We expect the test to reject this null-hypothesis, because we've created two very different distributions.  1\n2\n3\n4\n5 uv1   =   UncertainValue ( Normal ,   1.2 ,   0.3 )  uv2   =   UncertainValue ( Gamma ,   2 ,   3 )  # EqualVarianceTTest on 1000 draws for each variable  EqualVarianceTTest ( uv1 ,   uv2 ,   1000 )    The output is:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 Two   sample   t - test   ( equal   variance )  ----------------------------------  Population   details : \n     parameter   of   interest :     Mean   difference \n     value   under   h_0 :           0 \n     point   estimate :            - 4.782470406651697 \n     95 %   confidence   interval :   ( - 5.0428 ,   - 4.5222 )  Test   summary : \n     outcome   with   95 %   confidence :   reject   h_0 \n     two - sided   p - value :             1e-99  Details : \n     number   of   observations :     [ 1000 , 1000 ] \n     t - statistic :                - 36.03293014520585 \n     degrees   of   freedom :         1998 \n     empirical   standard   error :   0.1327249931487462    The test rejects the null-hypothesis, so we accept the alternative hypothesis that the samples come from distributions with different means and variances.", 
            "title": "Regular test"
        }, 
        {
            "location": "/hypothesistests/equal_variance_t_test/#pooled_test", 
            "text": "#  UncertainData.UncertainStatistics.EqualVarianceTTestPooled     Function .  1\n2 EqualVarianceTTestPooled ( d1 :: UncertainDataset ,   d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc 0 :: Real   =   0 )   -   EqualVarianceTTest    Consider two samples  s1[i]  and  s2[i] , each consisting of  n  random draws from the distributions furnishing the uncertain values  d1[i]  and  d2[i] , respectively. Gather all  s1[i]  in a pooled sample  S1 , and all  s2[i]  in a pooled sample  S2 .  Perform a two-sample t-test of the null hypothesis that  S1  and  S2  come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/hypothesistests/equal_variance_t_test/#element-wise_test", 
            "text": "#  UncertainData.UncertainStatistics.EqualVarianceTTestElementWise     Function .  1\n2 EqualVarianceTTestElementWise ( d1 :: UncertainDataset ,   d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc 0 :: Real   =   0 )   -   Vector { EqualVarianceTTest }    Consider two samples  s1[i]  and  s2[i] , each consisting of  n  random draws from the distributions furnishing the uncertain values  d1[i]  and  d2[i] , respectively. This function performs an elementwise  EqualVarianceTTest  on the pairs  (s1[i], s2[i]) . Specifically:  Performs an pairwise two-sample t-test of the null hypothesis that  s1[i]  and  s2[i]  come from distributions with equal means and variances against the alternative hypothesis that the distributions have different means but equal variances.  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/hypothesistests/unequal_variance_t_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests.UnequalVarianceTTest\n \n \nType\n.\n\n\n1\n2\nUnequalVarianceTTest\n(\nd1\n::\nAbstractUncertainValue\n,\n \nd2\n::\nAbstractUncertainValue\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc\n0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nUnequalVarianceTTest\n\n\n\n\n\n\n\nConsider two samples \ns1\n and \ns2\n, each consisting of \nn\n random draws from the distributions furnishing \nd1\n and \nd2\n, respectively.\n\n\nPerform an unequal variance two-sample t-test of the null hypothesis that \ns1\n and \ns2\n come from distributions with equal means against the alternative hypothesis that the distributions have different means.\n\n\nsource\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData.UncertainStatistics.UnequalVarianceTTestPooled\n \n \nFunction\n.\n\n\n1\n2\nUnequalVarianceTTestPooled\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc\n0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nUnequalVarianceTTest\n\n\n\n\n\n\n\nConsider two samples \ns1[i]\n and \ns2[i]\n, each consisting of \nn\n random draws from the distributions furnishing the uncertain values \nd1[i]\n and \nd2[i]\n, respectively. Gather all \ns1[i]\n in a pooled sample \nS1\n, and all \ns2[i]\n in a pooled sample \nS2\n.\n\n\nThis function performs an unequal variance two-sample t-test of the null hypothesis that \nS1\n and \nS2\n come from distributions with equal means against the alternative hypothesis that the distributions have different means.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData.UncertainStatistics.UnequalVarianceTTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nUnequalVarianceTTestElementWise\n(\nd1\n::\nUncertainDataset\n,\n \nd2\n::\nUncertainDataset\n,\n\n    \nn\n::\nInt\n \n=\n \n1000\n;\n \n\u03bc\n0\n::\nReal\n \n=\n \n0\n)\n \n-\n \nVector\n{\nUnequalVarianceTTest\n}\n\n\n\n\n\n\n\nConsider two samples \ns1[i]\n and \ns2[i]\n, each consisting of \nn\n random draws from the distributions furnishing the uncertain values \nd1[i]\n and \nd2[i]\n, respectively. This function performs an elementwise \nEqualVarianceTTest\n on the pairs \n(s1[i], s2[i])\n. Specifically:\n\n\nPerforms an pairwise unequal variance two-sample t-test of the null hypothesis that \ns1[i]\n and \ns2[i]\n come from distributions with equal means against the alternative hypothesis that the distributions have different means.\n\n\nThis test is sometimes known as Welch's t-test. It differs from the equal variance t-test in that it computes the number of degrees of freedom of the test using the Welch-Satterthwaite equation:\n\n\n\n\n\n    \u03bd_{\u03c7'} \u2248 \\frac{\\left(\\sum_{i=1}^n k_i s_i^2\\right)^2}{\\sum_{i=1}^n\n        \\frac{(k_i s_i^2)^2}{\u03bd_i}}\n\n\n\n\n    \u03bd_{\u03c7'} \u2248 \\frac{\\left(\\sum_{i=1}^n k_i s_i^2\\right)^2}{\\sum_{i=1}^n\n        \\frac{(k_i s_i^2)^2}{\u03bd_i}}\n\n\n\n\n\nsource", 
            "title": "Unequal variance t-test"
        }, 
        {
            "location": "/hypothesistests/unequal_variance_t_test/#regular_test", 
            "text": "#  HypothesisTests.UnequalVarianceTTest     Type .  1\n2 UnequalVarianceTTest ( d1 :: AbstractUncertainValue ,   d2 :: AbstractUncertainValue , \n     n :: Int   =   1000 ;   \u03bc 0 :: Real   =   0 )   -   UnequalVarianceTTest    Consider two samples  s1  and  s2 , each consisting of  n  random draws from the distributions furnishing  d1  and  d2 , respectively.  Perform an unequal variance two-sample t-test of the null hypothesis that  s1  and  s2  come from distributions with equal means against the alternative hypothesis that the distributions have different means.  source", 
            "title": "Regular test"
        }, 
        {
            "location": "/hypothesistests/unequal_variance_t_test/#pooled_test", 
            "text": "#  UncertainData.UncertainStatistics.UnequalVarianceTTestPooled     Function .  1\n2 UnequalVarianceTTestPooled ( d1 :: UncertainDataset ,   d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc 0 :: Real   =   0 )   -   UnequalVarianceTTest    Consider two samples  s1[i]  and  s2[i] , each consisting of  n  random draws from the distributions furnishing the uncertain values  d1[i]  and  d2[i] , respectively. Gather all  s1[i]  in a pooled sample  S1 , and all  s2[i]  in a pooled sample  S2 .  This function performs an unequal variance two-sample t-test of the null hypothesis that  S1  and  S2  come from distributions with equal means against the alternative hypothesis that the distributions have different means.  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/hypothesistests/unequal_variance_t_test/#element-wise_test", 
            "text": "#  UncertainData.UncertainStatistics.UnequalVarianceTTestElementWise     Function .  1\n2 UnequalVarianceTTestElementWise ( d1 :: UncertainDataset ,   d2 :: UncertainDataset , \n     n :: Int   =   1000 ;   \u03bc 0 :: Real   =   0 )   -   Vector { UnequalVarianceTTest }    Consider two samples  s1[i]  and  s2[i] , each consisting of  n  random draws from the distributions furnishing the uncertain values  d1[i]  and  d2[i] , respectively. This function performs an elementwise  EqualVarianceTTest  on the pairs  (s1[i], s2[i]) . Specifically:  Performs an pairwise unequal variance two-sample t-test of the null hypothesis that  s1[i]  and  s2[i]  come from distributions with equal means against the alternative hypothesis that the distributions have different means.  This test is sometimes known as Welch's t-test. It differs from the equal variance t-test in that it computes the number of degrees of freedom of the test using the Welch-Satterthwaite equation:   \n    \u03bd_{\u03c7'} \u2248 \\frac{\\left(\\sum_{i=1}^n k_i s_i^2\\right)^2}{\\sum_{i=1}^n\n        \\frac{(k_i s_i^2)^2}{\u03bd_i}}  \n    \u03bd_{\u03c7'} \u2248 \\frac{\\left(\\sum_{i=1}^n k_i s_i^2\\right)^2}{\\sum_{i=1}^n\n        \\frac{(k_i s_i^2)^2}{\u03bd_i}}   source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/hypothesistests/exact_kolmogorov_smirnov_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests.ExactOneSampleKSTest\n \n \nType\n.\n\n\n1\n2\nExactOneSampleKSTest(uv::AbstractUncertainValue,\n    d::UnivariateDistribution, n::Int = 1000) -\n ExactOneSampleKSTest\n\n\n\n\n\n\nPerform a one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that a draw of \nn\n realisations of the uncertain value \nuv\n comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource\n\n\nExample\n\n\nWe'll test whether the uncertain value \nuv = UncertainValue(Gamma, 2, 4)\n comes from the theoretical distribution \nGamma(2, 4)\n. Of course, we expect the test to confirm this, because we're using the exact same distribution.\n\n\n1\n2\n3\n4\n5\nuv\n \n=\n \nUncertainValue\n(\nGamma\n,\n \n2\n,\n \n4\n)\n\n\n\n# Perform the Kolgomorov-Smirnov test by drawing 1000 samples from the\n\n\n# uncertain value.\n\n\nExactOneSampleKSTest\n(\nuv\n,\n \nGamma\n(\n2\n,\n \n4\n),\n \n1000\n)\n\n\n\n\n\n\n\nThat gives the following output:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nExact\n \none\n \nsample\n \nKolmogorov\n-\nSmirnov\n \ntest\n\n\n----------------------------------------\n\n\nPopulation\n \ndetails\n:\n\n    \nparameter\n \nof\n \ninterest\n:\n   \nSupremum\n \nof\n \nCDF\n \ndifferences\n\n    \nvalue\n \nunder\n \nh_0\n:\n         \n0.0\n\n    \npoint\n \nestimate\n:\n          \n0.0228345021301449\n\n\n\nTest\n \nsummary\n:\n\n    \noutcome\n \nwith\n \n95\n%\n \nconfidence\n:\n \nfail\n \nto\n \nreject\n \nh_0\n\n    \ntwo\n-\nsided\n \np\n-\nvalue\n:\n           \n0.6655\n\n\n\nDetails\n:\n\n    \nnumber\n \nof\n \nobservations\n:\n   \n1000\n\n\n\n\n\n\n\nAs expected, the test can't reject the hypothesis that the uncertain value \nuv\n comes from the theoretical distribution \nGamma(2, 4)\n, precisely because it does.\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData.UncertainStatistics.ExactOneSampleKSTestPooled\n \n \nFunction\n.\n\n\n1\n2\nExactOneSampleKSTestPooled(ud::UncertainDataset,\n    d::UnivariateDistribution, n::Int = 1000) -\n ExactOneSampleKSTest\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n and pool them together. Then perform a one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that the pooled values comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData.UncertainStatistics.ExactOneSampleKSTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nExactOneSampleKSTestElementWise(ud::UncertainDataset,\n    d::UnivariateDistribution, n::Int = 1000) -\n Vector{ExactOneSampleKSTest}\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n, keeping one pool of values for each uncertain value.\n\n\nThen, perform an element-wise (pool-wise) one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that each value pool comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource", 
            "title": "Exact Kolmogorov-Smirnov test"
        }, 
        {
            "location": "/hypothesistests/exact_kolmogorov_smirnov_test/#regular_test", 
            "text": "#  HypothesisTests.ExactOneSampleKSTest     Type .  1\n2 ExactOneSampleKSTest(uv::AbstractUncertainValue,\n    d::UnivariateDistribution, n::Int = 1000) -  ExactOneSampleKSTest   Perform a one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that a draw of  n  realisations of the uncertain value  uv  comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source  Example  We'll test whether the uncertain value  uv = UncertainValue(Gamma, 2, 4)  comes from the theoretical distribution  Gamma(2, 4) . Of course, we expect the test to confirm this, because we're using the exact same distribution.  1\n2\n3\n4\n5 uv   =   UncertainValue ( Gamma ,   2 ,   4 )  # Perform the Kolgomorov-Smirnov test by drawing 1000 samples from the  # uncertain value.  ExactOneSampleKSTest ( uv ,   Gamma ( 2 ,   4 ),   1000 )    That gives the following output:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 Exact   one   sample   Kolmogorov - Smirnov   test  ----------------------------------------  Population   details : \n     parameter   of   interest :     Supremum   of   CDF   differences \n     value   under   h_0 :           0.0 \n     point   estimate :            0.0228345021301449  Test   summary : \n     outcome   with   95 %   confidence :   fail   to   reject   h_0 \n     two - sided   p - value :             0.6655  Details : \n     number   of   observations :     1000    As expected, the test can't reject the hypothesis that the uncertain value  uv  comes from the theoretical distribution  Gamma(2, 4) , precisely because it does.", 
            "title": "Regular test"
        }, 
        {
            "location": "/hypothesistests/exact_kolmogorov_smirnov_test/#pooled_test", 
            "text": "#  UncertainData.UncertainStatistics.ExactOneSampleKSTestPooled     Function .  1\n2 ExactOneSampleKSTestPooled(ud::UncertainDataset,\n    d::UnivariateDistribution, n::Int = 1000) -  ExactOneSampleKSTest   First, draw  n  realisations of each uncertain value in  ud  and pool them together. Then perform a one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that the pooled values comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/hypothesistests/exact_kolmogorov_smirnov_test/#element-wise_test", 
            "text": "#  UncertainData.UncertainStatistics.ExactOneSampleKSTestElementWise     Function .  1\n2 ExactOneSampleKSTestElementWise(ud::UncertainDataset,\n    d::UnivariateDistribution, n::Int = 1000) -  Vector{ExactOneSampleKSTest}   First, draw  n  realisations of each uncertain value in  ud , keeping one pool of values for each uncertain value.  Then, perform an element-wise (pool-wise) one-sample exact Kolmogorov\u2013Smirnov test of the null hypothesis that each value pool comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/hypothesistests/approximate_twosample_kolmogorov_smirnov_test/", 
            "text": "Pooled test\n\n\n#\n\n\nUncertainData.UncertainStatistics.ApproximateTwoSampleKSTestPooled\n \n \nFunction\n.\n\n\n1\n2\nApproximateTwoSampleKSTestPooled(d1::UncertainDataset,\n    d2::UncertainDataset, n::Int = 1000) -\n ApproximateTwoSampleKSTest\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nd1\n, then separately draw \nn\n realisations of each uncertain value in \nd2\n. Then, pool all realisations for \nd1\n together and all realisations of \nd2\n together.\n\n\nOn the pooled realisations, perform an asymptotic two-sample Kolmogorov\u2013Smirnov-test of the null hypothesis that the distribution furnishing the \nd1\n value pool represents the same distribution as the distribution furnishing the \nd2\n value pool, against the alternative hypothesis that the furnishing distributions are different.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData.UncertainStatistics.ApproximateTwoSampleKSTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nApproximateTwoSampleKSTestElementWise(d1::UncertainDataset,\n    d2::UncertainDataset, n::Int = 1000) -\n Vector{ApproximateTwoSampleKSTest}\n\n\n\n\n\n\nAssuming \nd1\n and \nd2\n contain the same number of uncertain observations, draw \nn\n realisations of each uncertain value in \nd1\n, then separately and separately draw \nn\n realisations of each uncertain value in \nd2\n.\n\n\nThen, perform an asymptotic two-sample Kolmogorov\u2013Smirnov-test of the null hypothesis that the uncertain values in \nd1\n and \nd2\n come from the same distribution against the alternative hypothesis that the (element-wise) values in  \nd1\n and \nd2\n come from different distributions.\n\n\nThe test is performed pairwise, i.e. ApproximateTwoSampleKSTest(d1[i], d2[i]) with \nn\n draws for the \ni\ni\n-ith pair of uncertain values.\n\n\nsource", 
            "title": "Approximate two-sample Kolmogorov-Smirnov test"
        }, 
        {
            "location": "/hypothesistests/approximate_twosample_kolmogorov_smirnov_test/#pooled_test", 
            "text": "#  UncertainData.UncertainStatistics.ApproximateTwoSampleKSTestPooled     Function .  1\n2 ApproximateTwoSampleKSTestPooled(d1::UncertainDataset,\n    d2::UncertainDataset, n::Int = 1000) -  ApproximateTwoSampleKSTest   First, draw  n  realisations of each uncertain value in  d1 , then separately draw  n  realisations of each uncertain value in  d2 . Then, pool all realisations for  d1  together and all realisations of  d2  together.  On the pooled realisations, perform an asymptotic two-sample Kolmogorov\u2013Smirnov-test of the null hypothesis that the distribution furnishing the  d1  value pool represents the same distribution as the distribution furnishing the  d2  value pool, against the alternative hypothesis that the furnishing distributions are different.  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/hypothesistests/approximate_twosample_kolmogorov_smirnov_test/#element-wise_test", 
            "text": "#  UncertainData.UncertainStatistics.ApproximateTwoSampleKSTestElementWise     Function .  1\n2 ApproximateTwoSampleKSTestElementWise(d1::UncertainDataset,\n    d2::UncertainDataset, n::Int = 1000) -  Vector{ApproximateTwoSampleKSTest}   Assuming  d1  and  d2  contain the same number of uncertain observations, draw  n  realisations of each uncertain value in  d1 , then separately and separately draw  n  realisations of each uncertain value in  d2 .  Then, perform an asymptotic two-sample Kolmogorov\u2013Smirnov-test of the null hypothesis that the uncertain values in  d1  and  d2  come from the same distribution against the alternative hypothesis that the (element-wise) values in   d1  and  d2  come from different distributions.  The test is performed pairwise, i.e. ApproximateTwoSampleKSTest(d1[i], d2[i]) with  n  draws for the  i i -ith pair of uncertain values.  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/hypothesistests/jarque_bera_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests.JarqueBeraTest\n \n \nType\n.\n\n\n1\nJarqueBeraTest(d::AbstractUncertainValue, n::Int = 1000) -\n JarqueBeraTest\n\n\n\n\n\n\nCompute the Jarque-Bera statistic to test the null hypothesis that an uncertain value is normally distributed.\n\n\nsource\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData.UncertainStatistics.JarqueBeraTestPooled\n \n \nFunction\n.\n\n\n1\nJarqueBeraTestPooled(ud::UncertainDataset, n::Int = 1000) -\n JarqueBeraTest\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n and pool them together. Then, compute the Jarque-Bera statistic to test the null hypothesis that the values of the pool are normally distributed.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData.UncertainStatistics.JarqueBeraTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nOneSampleADTestElementWise(ud::UncertainDataset,\n    n::Int = 1000) -\n Vector{JarqueBeraTest}\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n, keeping one pool of values for each uncertain value.\n\n\nThen, compute the Jarque-Bera statistic to test the null hypothesis that each value pool is normally distributed.\n\n\nsource", 
            "title": "Jarque-Bera test"
        }, 
        {
            "location": "/hypothesistests/jarque_bera_test/#regular_test", 
            "text": "#  HypothesisTests.JarqueBeraTest     Type .  1 JarqueBeraTest(d::AbstractUncertainValue, n::Int = 1000) -  JarqueBeraTest   Compute the Jarque-Bera statistic to test the null hypothesis that an uncertain value is normally distributed.  source", 
            "title": "Regular test"
        }, 
        {
            "location": "/hypothesistests/jarque_bera_test/#pooled_test", 
            "text": "#  UncertainData.UncertainStatistics.JarqueBeraTestPooled     Function .  1 JarqueBeraTestPooled(ud::UncertainDataset, n::Int = 1000) -  JarqueBeraTest   First, draw  n  realisations of each uncertain value in  ud  and pool them together. Then, compute the Jarque-Bera statistic to test the null hypothesis that the values of the pool are normally distributed.  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/hypothesistests/jarque_bera_test/#element-wise_test", 
            "text": "#  UncertainData.UncertainStatistics.JarqueBeraTestElementWise     Function .  1\n2 OneSampleADTestElementWise(ud::UncertainDataset,\n    n::Int = 1000) -  Vector{JarqueBeraTest}   First, draw  n  realisations of each uncertain value in  ud , keeping one pool of values for each uncertain value.  Then, compute the Jarque-Bera statistic to test the null hypothesis that each value pool is normally distributed.  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/hypothesistests/mann_whitney_u_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests.MannWhitneyUTest\n \n \nFunction\n.\n\n\n1\n2\nMannWhitneyUTest(d1::AbstractUncertainValue, d2::AbstractUncertainValue,\n    n::Int = 1000) -\n MannWhitneyUTest\n\n\n\n\n\n\nLet \ns1\n and \ns2\n be samples of \nn\n realisations from the distributions furnishing the uncertain values \nd1\n and \nd2\n.\n\n\nPerform a Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as \ns1\n is greater than an observation drawn from the same population as \ns2\n is equal to the probability that an observation drawn from the same population as \ns2\n is greater than an observation drawn from the same population as \ns1\n against the alternative hypothesis that these probabilities are not equal.\n\n\nThe Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples, \nMannWhitneyUTest\n performs an exact Mann-Whitney U test. In all other cases, \nMannWhitneyUTest\n performs an approximate Mann-Whitney U test.\n\n\nsource\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData.UncertainStatistics.MannWhitneyUTestPooled\n \n \nFunction\n.\n\n\n1\n2\nMannWhitneyUTest(d1::UncertainDataset, d2::UncertainDataset,\n    n::Int = 1000) -\n MannWhitneyUTest\n\n\n\n\n\n\nLet \ns_{1_{i}}\ns_{1_{i}}\n be a sample of \nn\n realisations of the distribution furnishing the uncertain value \nd1[i]\n, where \ni \\in [1, 2, \\ldots, N]\ni \\in [1, 2, \\ldots, N]\n and \nN\nN\n is the number of uncertain values in \nd1\n.  Next, gather the samples for all \ns_{1_{i}}\ns_{1_{i}}\n in a pooled sample \nS_1\nS_1\n.  Do the same for the second uncertain dataset \nd2\n, yielding the pooled sample  \nS_2\nS_2\n.\n\n\nPerform a Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as \nS_1\nS_1\n is greater than an observation drawn from the same population as \nS_2\nS_2\n is equal to the probability that an observation drawn from the same population as \nS_2\nS_2\n is greater than an observation drawn from the same population as \nS_1\nS_1\n against the alternative hypothesis that these probabilities are not equal.\n\n\nThe Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples, \nMannWhitneyUTest\n performs an exact Mann-Whitney U test. In all other cases, \nMannWhitneyUTest\n performs an approximate Mann-Whitney U test.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData.UncertainStatistics.MannWhitneyUTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nMannWhitneyUTest(d1::UncertainDataset, d2::UncertainDataset,\n    n::Int = 1000) -\n Vector{MannWhitneyUTest}\n\n\n\n\n\n\nAssume \nd1\n and \nd2\n consist of the same number of uncertain values. Let \ns_{1_{i}}\ns_{1_{i}}\n be a sample of \nn\n realisations of the distribution furnishing the uncertain value \nd1[i]\n, where \ni \\in [1, 2, \\ldots, N]\ni \\in [1, 2, \\ldots, N]\n and \nN\nN\n is the number of uncertain values in \nd1\n. Let \ns_{2_{i}}\ns_{2_{i}}\n be the corresponding sample for \nd2[i]\n. This function\n\n\nPerform an element-wise Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as \ns_{1_{i}}\ns_{1_{i}}\n is greater than an observation drawn from the same population as \ns_{2_{i}}\ns_{2_{i}}\n is equal to the probability that an observation drawn from the same population as \ns_{2_{i}}\ns_{2_{i}}\n is greater than an observation drawn from the same population as \ns_{1_{i}}\ns_{1_{i}}\n against the alternative hypothesis that these probabilities are not equal.\n\n\nThe Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples, \nMannWhitneyUTest\n performs an exact Mann-Whitney U test. In all other cases, \nMannWhitneyUTest\n performs an approximate Mann-Whitney U test.\n\n\nsource", 
            "title": "Mann-Whitney u-test"
        }, 
        {
            "location": "/hypothesistests/mann_whitney_u_test/#regular_test", 
            "text": "#  HypothesisTests.MannWhitneyUTest     Function .  1\n2 MannWhitneyUTest(d1::AbstractUncertainValue, d2::AbstractUncertainValue,\n    n::Int = 1000) -  MannWhitneyUTest   Let  s1  and  s2  be samples of  n  realisations from the distributions furnishing the uncertain values  d1  and  d2 .  Perform a Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as  s1  is greater than an observation drawn from the same population as  s2  is equal to the probability that an observation drawn from the same population as  s2  is greater than an observation drawn from the same population as  s1  against the alternative hypothesis that these probabilities are not equal.  The Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples,  MannWhitneyUTest  performs an exact Mann-Whitney U test. In all other cases,  MannWhitneyUTest  performs an approximate Mann-Whitney U test.  source", 
            "title": "Regular test"
        }, 
        {
            "location": "/hypothesistests/mann_whitney_u_test/#pooled_test", 
            "text": "#  UncertainData.UncertainStatistics.MannWhitneyUTestPooled     Function .  1\n2 MannWhitneyUTest(d1::UncertainDataset, d2::UncertainDataset,\n    n::Int = 1000) -  MannWhitneyUTest   Let  s_{1_{i}} s_{1_{i}}  be a sample of  n  realisations of the distribution furnishing the uncertain value  d1[i] , where  i \\in [1, 2, \\ldots, N] i \\in [1, 2, \\ldots, N]  and  N N  is the number of uncertain values in  d1 .  Next, gather the samples for all  s_{1_{i}} s_{1_{i}}  in a pooled sample  S_1 S_1 .  Do the same for the second uncertain dataset  d2 , yielding the pooled sample   S_2 S_2 .  Perform a Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as  S_1 S_1  is greater than an observation drawn from the same population as  S_2 S_2  is equal to the probability that an observation drawn from the same population as  S_2 S_2  is greater than an observation drawn from the same population as  S_1 S_1  against the alternative hypothesis that these probabilities are not equal.  The Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples,  MannWhitneyUTest  performs an exact Mann-Whitney U test. In all other cases,  MannWhitneyUTest  performs an approximate Mann-Whitney U test.  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/hypothesistests/mann_whitney_u_test/#element-wise_test", 
            "text": "#  UncertainData.UncertainStatistics.MannWhitneyUTestElementWise     Function .  1\n2 MannWhitneyUTest(d1::UncertainDataset, d2::UncertainDataset,\n    n::Int = 1000) -  Vector{MannWhitneyUTest}   Assume  d1  and  d2  consist of the same number of uncertain values. Let  s_{1_{i}} s_{1_{i}}  be a sample of  n  realisations of the distribution furnishing the uncertain value  d1[i] , where  i \\in [1, 2, \\ldots, N] i \\in [1, 2, \\ldots, N]  and  N N  is the number of uncertain values in  d1 . Let  s_{2_{i}} s_{2_{i}}  be the corresponding sample for  d2[i] . This function  Perform an element-wise Mann-Whitney U test of the null hypothesis that the probability that an observation drawn from the same population as  s_{1_{i}} s_{1_{i}}  is greater than an observation drawn from the same population as  s_{2_{i}} s_{2_{i}}  is equal to the probability that an observation drawn from the same population as  s_{2_{i}} s_{2_{i}}  is greater than an observation drawn from the same population as  s_{1_{i}} s_{1_{i}}  against the alternative hypothesis that these probabilities are not equal.  The Mann-Whitney U test is sometimes known as the Wilcoxon rank-sum test. When there are no tied ranks and \u226450 samples, or tied ranks and \u226410 samples,  MannWhitneyUTest  performs an exact Mann-Whitney U test. In all other cases,  MannWhitneyUTest  performs an approximate Mann-Whitney U test.  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/hypothesistests/anderson_darling_test/", 
            "text": "Regular test\n\n\n#\n\n\nHypothesisTests.OneSampleADTest\n \n \nType\n.\n\n\n1\n2\nOneSampleADTest(uv::UncertainValue, d::UnivariateDistribution,\n    n::Int = 1000) -\n OneSampleADTest\n\n\n\n\n\n\nPerform a one-sample Anderson\u2013Darling test of the null hypothesis that a draw of \nn\n realisations of the uncertain value \nuv\n comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource\n\n\n\n\nPooled test\n\n\n#\n\n\nUncertainData.UncertainStatistics.OneSampleADTestPooled\n \n \nFunction\n.\n\n\n1\n2\nOneSampleADTestPooled(ud::UncertainDataset, d::UnivariateDistribution,\n    n::Int = 1000)) -\n OneSampleADTest\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n and pool them together. Then perform a one-sample Anderson\u2013Darling test of the null hypothesis that the pooled values comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource\n\n\n\n\nElement-wise test\n\n\n#\n\n\nUncertainData.UncertainStatistics.OneSampleADTestElementWise\n \n \nFunction\n.\n\n\n1\n2\nOneSampleADTestElementWise(ud::UncertainDataset, d::UnivariateDistribution,\n    n::Int = 1000)) -\n Vector{OneSampleADTest}\n\n\n\n\n\n\nFirst, draw \nn\n realisations of each uncertain value in \nud\n, keeping one pool of values for each uncertain value. Then, perform an element-wise (pool-wise) one-sample Anderson\u2013Darling test of the null hypothesis that each value pool comes from the distribution \nd\n against the alternative hypothesis that the sample is not drawn from \nd\n.\n\n\nsource", 
            "title": "Anderson-Darling test"
        }, 
        {
            "location": "/hypothesistests/anderson_darling_test/#regular_test", 
            "text": "#  HypothesisTests.OneSampleADTest     Type .  1\n2 OneSampleADTest(uv::UncertainValue, d::UnivariateDistribution,\n    n::Int = 1000) -  OneSampleADTest   Perform a one-sample Anderson\u2013Darling test of the null hypothesis that a draw of  n  realisations of the uncertain value  uv  comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source", 
            "title": "Regular test"
        }, 
        {
            "location": "/hypothesistests/anderson_darling_test/#pooled_test", 
            "text": "#  UncertainData.UncertainStatistics.OneSampleADTestPooled     Function .  1\n2 OneSampleADTestPooled(ud::UncertainDataset, d::UnivariateDistribution,\n    n::Int = 1000)) -  OneSampleADTest   First, draw  n  realisations of each uncertain value in  ud  and pool them together. Then perform a one-sample Anderson\u2013Darling test of the null hypothesis that the pooled values comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source", 
            "title": "Pooled test"
        }, 
        {
            "location": "/hypothesistests/anderson_darling_test/#element-wise_test", 
            "text": "#  UncertainData.UncertainStatistics.OneSampleADTestElementWise     Function .  1\n2 OneSampleADTestElementWise(ud::UncertainDataset, d::UnivariateDistribution,\n    n::Int = 1000)) -  Vector{OneSampleADTest}   First, draw  n  realisations of each uncertain value in  ud , keeping one pool of values for each uncertain value. Then, perform an element-wise (pool-wise) one-sample Anderson\u2013Darling test of the null hypothesis that each value pool comes from the distribution  d  against the alternative hypothesis that the sample is not drawn from  d .  source", 
            "title": "Element-wise test"
        }, 
        {
            "location": "/resampling/", 
            "text": "Resampling\n\n\n\n\nWithout constraints\n\n\nRegular resampling is done by drawing random number from the entire probability distributions furnishing the uncertain values.\n\n\n\n\nWith constraints\n\n\nThe following syntax is used to resample uncertain values.\n\n\n\n\nresample(uv::AbstractUncertainValue, constraint::SamplingConstraint)\n. Resample the uncertain value once within the restrictions imposed by the sampling constraint.\n\n\nresample(uv::AbstractUncertainValue, constraint::SamplingConstraint, n::Int)\n. Resample the uncertain value \nn\n times within the restrictions imposed by the sampling constraint.\n\n\n\n\n\n\nSampling constraints\n\n\nThe following sampling constraints are available:\n\n\n\n\nTruncateStd(n\u03c3::Int)\n. Truncate the distribution furnishing the uncertain data point(s) at n times the standard deviation of the distribution.\n\n\nTruncateMinimum(min\n:Number)\n. Truncate the distribution furnishing the uncertain data point(s) at some minimum value.\n\n\nTruncateMaximum(max\n:Number)\n. Truncate the distribution furnishing the uncertain data point(s) at some maximum value.\n\n\nTruncateRange(min\n:Number, max\n:Number)\n. Truncate the distribution furnishing the uncertain data point(s) at some range.\n\n\nTruncateLowerQuantile(lower_quantile::Float64)\n. Truncate the distribution furnishing the uncertain data point(s) at some lower quantile of the distribution.\n\n\nTruncateUpperQuantile(upper_quantile::Float64)\n. Truncate the distribution furnishing the uncertain data point(s) at some upper quantile of the distribution.\n\n\nTruncateQuantiles(lower_quantile::Float64, upper_quantile::Float64)\n. Truncate the distribution furnishing the uncertain data point(s) at a \nlower_quantile\n\n\n\n\nand an \nupper_quantile\n of the distribution.\n\n\n\n\nExamples\n\n\nLet \nuv = UncertainValue(Normal, 1, 0.2)\n. One may, for example, impose the following sampling constraints:\n\n\n\n\nresample(uv, TruncateLowerQuantile(0.2))\n. Resamples \nuv\n 100 times, drawing values strictly larger than the 0.2-th quantile of the distribution furnishing the uncertain data point.\n\n\nresample(uv, TruncateStd(1), 100)\n. Resamples \nuv\n 100 times, drawing values falling within one standard deviation of the distribution furnishing the uncertain value.\n\n\nresample(uv, TruncateRange(-0.5, 1), 100)\n. Resamples \nuv\n 100 times, drawing values from the distribution furnishing the uncertain value within the interval \n[-0.5, 1]\n.", 
            "title": "Resampling"
        }, 
        {
            "location": "/resampling/#resampling", 
            "text": "", 
            "title": "Resampling"
        }, 
        {
            "location": "/resampling/#without_constraints", 
            "text": "Regular resampling is done by drawing random number from the entire probability distributions furnishing the uncertain values.", 
            "title": "Without constraints"
        }, 
        {
            "location": "/resampling/#with_constraints", 
            "text": "The following syntax is used to resample uncertain values.   resample(uv::AbstractUncertainValue, constraint::SamplingConstraint) . Resample the uncertain value once within the restrictions imposed by the sampling constraint.  resample(uv::AbstractUncertainValue, constraint::SamplingConstraint, n::Int) . Resample the uncertain value  n  times within the restrictions imposed by the sampling constraint.", 
            "title": "With constraints"
        }, 
        {
            "location": "/resampling/#sampling_constraints", 
            "text": "The following sampling constraints are available:   TruncateStd(n\u03c3::Int) . Truncate the distribution furnishing the uncertain data point(s) at n times the standard deviation of the distribution.  TruncateMinimum(min :Number) . Truncate the distribution furnishing the uncertain data point(s) at some minimum value.  TruncateMaximum(max :Number) . Truncate the distribution furnishing the uncertain data point(s) at some maximum value.  TruncateRange(min :Number, max :Number) . Truncate the distribution furnishing the uncertain data point(s) at some range.  TruncateLowerQuantile(lower_quantile::Float64) . Truncate the distribution furnishing the uncertain data point(s) at some lower quantile of the distribution.  TruncateUpperQuantile(upper_quantile::Float64) . Truncate the distribution furnishing the uncertain data point(s) at some upper quantile of the distribution.  TruncateQuantiles(lower_quantile::Float64, upper_quantile::Float64) . Truncate the distribution furnishing the uncertain data point(s) at a  lower_quantile   and an  upper_quantile  of the distribution.", 
            "title": "Sampling constraints"
        }, 
        {
            "location": "/resampling/#examples", 
            "text": "Let  uv = UncertainValue(Normal, 1, 0.2) . One may, for example, impose the following sampling constraints:   resample(uv, TruncateLowerQuantile(0.2)) . Resamples  uv  100 times, drawing values strictly larger than the 0.2-th quantile of the distribution furnishing the uncertain data point.  resample(uv, TruncateStd(1), 100) . Resamples  uv  100 times, drawing values falling within one standard deviation of the distribution furnishing the uncertain value.  resample(uv, TruncateRange(-0.5, 1), 100) . Resamples  uv  100 times, drawing values from the distribution furnishing the uncertain value within the interval  [-0.5, 1] .", 
            "title": "Examples"
        }, 
        {
            "location": "/implementing_algorithms_for_uncertaindata/", 
            "text": "Extending existing algorithms for uncertain data types\n\n\nDo you already have an algorithm computing some statistic that you want to obtain uncertainty estimates for? Simply use Julia's multiple dispatch and create a version of the algorithm function that accepts the \nAbstractUncertainValue\n and \nAbstractUncertainDataset\n types, along with a \nSamplingConstraints\n specifying how the uncertain values are should be resampled.\n\n\nA basic function skeleton could be\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n# Some algorithm computing a statistic for a scalar-valued vector\n\n\nfunction\n \nmyalgorithm\n(\ndataset\n::\nVector\n{\nT\n};\n \nkwargs\n...\n)\n \nwhere\n \nT\n\n    \n# some algorithm returning a single-valued statistic\n\n\nend\n\n\n\n# Applying the algorithm to an ensemble of realisations from\n\n\n# an uncertain dataset, given a sampling constraint.\n\n\nfunction\n \nmyalgorithm\n(\nd\n::\nUncertainDataset\n,\n \nconstraint\n::\nC\n;\n\n        \nn_ensemble_realisations\n \n=\n \n100\n,\n \nkwargs\n...\n)\n\n        \nwhere\n \n{\nC\n \n:\n \nSamplingConstraint\n}\n\n\n    \nensemble_stats\n \n=\n \nzeros\n(\nn_ensemble_realisations\n)\n\n\n    \nfor\n \ni\n \nin\n \n1\n:\nn_ensemble_realisations\n\n        \nensemble_stats\n[\ni\n]\n \n=\n \nmyalgorithm\n(\nresample\n(\nd\n,\n \nconstraint\n);\n \nkwargs\n...\n)\n\n    \nend\n\n\n    \nreturn\n \nensemble_stats\n\n\nend", 
            "title": "Implementing algorithms for uncertain data"
        }, 
        {
            "location": "/implementing_algorithms_for_uncertaindata/#extending_existing_algorithms_for_uncertain_data_types", 
            "text": "Do you already have an algorithm computing some statistic that you want to obtain uncertainty estimates for? Simply use Julia's multiple dispatch and create a version of the algorithm function that accepts the  AbstractUncertainValue  and  AbstractUncertainDataset  types, along with a  SamplingConstraints  specifying how the uncertain values are should be resampled.  A basic function skeleton could be   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 # Some algorithm computing a statistic for a scalar-valued vector  function   myalgorithm ( dataset :: Vector { T };   kwargs ... )   where   T \n     # some algorithm returning a single-valued statistic  end  # Applying the algorithm to an ensemble of realisations from  # an uncertain dataset, given a sampling constraint.  function   myalgorithm ( d :: UncertainDataset ,   constraint :: C ; \n         n_ensemble_realisations   =   100 ,   kwargs ... ) \n         where   { C   :   SamplingConstraint } \n\n     ensemble_stats   =   zeros ( n_ensemble_realisations ) \n\n     for   i   in   1 : n_ensemble_realisations \n         ensemble_stats [ i ]   =   myalgorithm ( resample ( d ,   constraint );   kwargs ... ) \n     end \n\n     return   ensemble_stats  end", 
            "title": "Extending existing algorithms for uncertain data types"
        }
    ]
}